[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I work in the financial services sector as a Business Intelligence & Analytics Lead. Previously, I worked for mPharma as a Data Management Associate.\nIn another life, I was an Accounting and Finance major. Graduated from the University of Namibia with a BA degree in Accounting(Honours). Equally tutored Business Administration students in the art of business and accounting at the University of Zambia.\nI am lost in the Data Science web… hoping to find my way out of the maze soon. I find data and the process of data analysis to glean insight from the raw data, overly exciting and fascinating alike.\nIn another world, I’m fascinated by stock markets and everything with a business and finance buzz.\nI can be found on Github poking around interesting projects… and occasionally I rant on twitter.\nFor this website\n\nThis will be a place where I share my daily musings in the world of R, analytics and mostly, everything remotely interesting. I have a knack for learning interesting data analytics technologies.\n\nLet's have fun! :smile:\nRecommended blogs\n\nYihui Xie\nDavid Robinson - Variance explained\nSimply Statistics\nR Bloggers\nRStudio blog\nRStudio community: Everything RStudio\nR Weekly"
  },
  {
    "objectID": "accomplishments.html",
    "href": "accomplishments.html",
    "title": "Accomplishments",
    "section": "",
    "text": "Professional Certifications\n\nMicrosoft Professional Program in Data Science – Certification obtained in January 2020\nData Engineering with Google Cloud - April 2020 (Data engineering, smart analytics, artificial intelligence, and data warehousing using BigQuery)\nGoogle Developers Certification for Google Cloud - A recipient of the Google Africa Developer Scholarship for the year 2020.\nFull-Stack Software Engineer 2022 - Currently pursuing this programme under the ALX Africa\n\n\n\nAcademic Career\nUniversity of Namibia - Bachelor of Accounting (Honours)\n\n\nCourses completed\n\nData Analysis with Spreadsheets - DataCamp\nIntroduction to R course - DataCamp\nIntroduction to SQL for Data Science Course - DataCamp\nIntroduction to the Tidyverse - DataCamp\nReporting with R Markdown - DataCamp\nIntroduction to Git for Data Science - DataCamp\nIntro to Statistics with R: Introduction DataCamp\nSpreadsheet Basics DataCamp\nIntroduction to Data and Probability Coursera\nLearn the Command Line Codeacademy"
  },
  {
    "objectID": "blog/afrimarkets/index.html",
    "href": "blog/afrimarkets/index.html",
    "title": "African Markets indices tracker",
    "section": "",
    "text": "The goal is to keep this updated with data mined to Google sheets. I have leveraged on Google Data Studio for the visualizations and its continuous integration to Google sheets to pull the latest data as it becomes available.\n\n\n\nCitationBibTeX citation:@online{simumba2018,\n  author = {Aaron Simumba},\n  title = {African {Markets} Indices Tracker},\n  date = {2018-11-07},\n  url = {https://asimumba.rbind.io//blog/afrimarkets},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nAaron Simumba. 2018. “African Markets Indices Tracker.”\nNovember 7, 2018. https://asimumba.rbind.io//blog/afrimarkets."
  },
  {
    "objectID": "blog/blogdown/index.html",
    "href": "blog/blogdown/index.html",
    "title": "Building a Website with Blogdown and Hugo",
    "section": "",
    "text": "For starters, there is offical documentation which is so far still under developement, but nonetheless it has already covered most of the pertinent issues to get you started buiding your own desired website.\nIf you are coming from the Google blogspot or wordpress dungeon, then by default I will assume you are going to love this new introduction to the blogsphere. It’s worth to make mention though that this, may not look quite easy and appealing to people with very little computers/tech aquaintance. For your own information, I am an Accounting/finace major in another life; apparently things had to balance, maybe so, some of the world’s things have to find a reconciliation point after all. This I hope if you’ve never built a website or even a blog it will help you believe you can do it, just the way I am doing it!\nEnough with the stories, buckle up we jump right into it.\nFor the theme, I was initially spoiled for choice, I tried a few before finally settling for the Cactus theme. It’s both well enriched with great visual aesthetics as well as a clean presentation. I have changed quite a lot of stuff to meet my own preferences and for what I intend to use the website for. If your choice is to get up and running without bothering to learn a bit of HTML and CSS for further alterations and customizations, the Hugo site has plenty of templates to suit different tastes.\n\nNOTE: Lookout for a comprehensive summary of how I designed my current website soon…\n\n\n\nCitationBibTeX citation:@online{simumba2017,\n  author = {Aaron Simumba},\n  title = {Building a {Website} with {Blogdown} and {Hugo}},\n  date = {2017-07-03},\n  url = {https://asimumba.rbind.io//blog/blogdown},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nAaron Simumba. 2017. “Building a Website with Blogdown and\nHugo.” July 3, 2017. https://asimumba.rbind.io//blog/blogdown."
  },
  {
    "objectID": "blog/datahack4fi/index.html",
    "href": "blog/datahack4fi/index.html",
    "title": "DataHack4Fi twitter data",
    "section": "",
    "text": "CitationBibTeX citation:@online{simumba2018,\n  author = {Aaron Simumba},\n  title = {DataHack4Fi Twitter Data},\n  date = {2018-11-11},\n  url = {https://asimumba.rbind.io//blog/datahack4fi},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nAaron Simumba. 2018. “DataHack4Fi Twitter Data.” November\n11, 2018. https://asimumba.rbind.io//blog/datahack4fi."
  },
  {
    "objectID": "blog/luse/index.html",
    "href": "blog/luse/index.html",
    "title": "What’s in the numbers: A look at the Lusaka Securities Exchange",
    "section": "",
    "text": "In the next coming few weeks, I plan to do a series of posts specifically targeted at the Lusaka Securities Exchange (LuSE). Essentially, this idea came to mind after reading through this Bloomberg Markets article, which highlights historical equity market returns . One aspect that stands out from this article is that over the past 2 decades, emerging markets equities provided greater returns; both on a risk adjusted basis and in dollar terms.\nAnd of focus here, the LuSE, ranks in the top 10 on both account of returns after converting to US dollars and on a risk adjusted basis. The table below shows the local bourse appearing twice in the top ten, twice on number 2 in the year 2007 and 2011 only being edge out of the first spot by Mongolia on both occasions.\nAs this data from Bloomberg Markets shows, emerging and frontier markets accounted for 9 spots out of a rank order of 10 in dollar terms and 7 spots on a risk adjusted basis.\n\n\n\nSource: Bloomberg\n\n\nThe following excerpt from Bloomberg stands out:\n\n“The top 10 stock indexes that gave global investors the greatest returns in 2016 were all emerging or frontier markets, according to data compiled by Bloomberg. That’s not a fluke: over the past 20 years, nine out of 10 best-performing equity gauges have been in developing nations, the figures show”.\n\n\n\n\nInvestment theory contends that risk taken in an investment must commensurate the return sought. Though equities as a class of investable assets are by their very nature riskier assets relative to other assets like Treasury Bill or Bonds; this in essence frightens most investors to invest in them. More so from the view of the emerging markets, which expose investors to higher levels of volatility relative to more mature and developed markets. In my next post I will look at what was historically being demanded by investors for taking up the extra risk posed by emerging markets.\nThe table below shows the top 10 market returns on a risk adjusted basis. No doubt emerging markets still dominated the list.\n\n\n\nSource: Bloomberg\n\n\nWhat’s impressive from this table is that from the period 1997-2016, Zambia appears 5 times, occupying the first position once in the year 2013. The country appeared on 4th position within the top 5 spots in terms of risk adjusted equity returns in the world .In local currency terms this reflected an over 40% return as can be seen from figure @ref(fig:first-figure) below. Going forward, investors can remain optimistic on the prospects of the emerging markets delivering stellar returns ahead of the developed markets pack. So far, the MSCI Emerging Markets Index has gained 2.7 percent in comparison with 1.5 percent from developed stock markets.\n\nIn focus\nThe following charts give a snapshot of the performance of the Lusaka Securities Exchange All Share index (LASI). For the monthly index data, the period covered is from January 2005 to December 2014. As for the yearly data, it goes all the way back to January 1997- December 2014. Furthermore, I present the data on both time intervals, inclusive and exclusive of the Zambia Consolidated Copper Mines Investment Holdings (ZCCM-IH). This is simply to highlight the effect that the company has on the market index as a whole and on the bigger side, the nation’s economy.\nNB: The returns are computed in the local currency (Kwacha).\n\nlibrary(dygraphs)\n\ndygraph((LASI_monthly$Return) * 100,\n        main = \"LASI Monthly Returns 2005 - 2014 Excl. ZCCM - IH\") %>%\n  \n  dyAxis(\"y\", label = \"Return\") %>%\n  \n  dyAxis(\"x\", label = \"Year\") %>%\n  \n  dyOptions(colors = RColorBrewer::brewer.pal(3, \"Set2\")) \n\n\n\nThe LuSE LASI Monthly Returns For the Period 2005-2014 (excl. ZCCM-IH)\n\n\nMost noticeable from the above chart is the greater volatility experienced at the on set of the Global financial crisis. At the height of this crisis we see the index slumping close to -40.0%, conversely, also it hit a record high of more than 60.0% return. The index overall performance is an up-down swing with periods of decent returns and other periods of downward performance.\n\n\nMonthly Returns Inclusive of ZCCM-IH\nThe following chart shows the LASI index inclusive of ZCCM-IH. Since the LASI is a value weighted index, stocks having greater market value tend to have a much more pronounced effect on the movement in the index. For instance, factoring ZCCM-IH’s market capitalization into the LASI index has a telling effect on the overall picture of the index. The chart below shows volatility in certain months relatively lower compared to the chart above which excludes ZCCM-IH. My follow-up posts will closely evaluate this effect and try to elicit some information that explains why this is the case.\n\ndygraph((LASI_monthly_zccm$Return) * 100,\n        main = \"LASI Monthly Returns 2005 - 2014 Inc. ZCCM-IH\") %>%\n  \n  dyAxis(\"y\", label = \"Return\") %>%\n  \n  dyAxis(\"x\", label = \"Year\") %>%\n  \n  dyOptions(colors = RColorBrewer::brewer.pal(3, \"Set2\")) \n\n\n\nThe LuSE LASI Monthly Returns For the Period 2005-2014 (incl. ZCCM-IH)\n\n\nThe last two charts detail the index on a yearly basis. Though unlike the picture painted from the monthly data, yearly data that goes farther down the years, seem to be carrying a different message on the effect of the company ZCCM-IH. When introduced into the computations, the volatility is more than that observed without its inclusion.\n\ndygraph((LASI_yearly$Return)*100,\n        main = \"LASI Monthly Returns 2005 - 2014 Inc. ZCCM-IH\") %>% \n   \n  dyAxis(\"y\", label =\"Return\") %>%\n   \n  dyAxis(\"x\", label =\"Year\") %>%\n   \n  dyOptions(colors = RColorBrewer::brewer.pal(3, \"Set2\")) \n\n\n\nThe LuSE LASI yearly Returns For the Period 1997-2014 (excl. ZCCM-IH)\n\n\nInclusive of ZCCM-IH yearly LASI Index returns\n\ndygraph((LASI_yearly_zccm$Return) * 100,\n        main = \"LASI Monthly Returns 2005 - 2014 Inc. ZCCM-IH\") %>%\n  \n  dyAxis(\"y\", label = \"Return\") %>%\n  \n  dyAxis(\"x\", label = \"Year\") %>%\n  \n  dyOptions(colors = RColorBrewer::brewer.pal(3, \"Set2\")) \n\n\n\nThe LuSE LASI yearly Returns For the Period 1997-2014 (incl. ZCCM-IH)\n\n\n\n\nFinal Remarks\nIt’s with no doubt the Emerging Markets had a stellar performance the past 2 decades. But what remains inherently attached with these developing markets is: (i) lack of proper oversight on market monitoring and supervision; (ii) the lack of or inadequate liquidity in the market among other challenges. In the coming posts I hope to dwell more on the performance of the LuSE and have a look at its peer exchanges in the region.\n[For the data and the code,you can find them on my GitHub account.]\n\n\n\n\nCitationBibTeX citation:@online{simumba2017,\n  author = {Aaron Simumba},\n  title = {What’s in the Numbers: {A} Look at the {Lusaka} {Securities}\n    {Exchange}},\n  date = {2017-04-06},\n  url = {https://asimumba.rbind.io//blog/luse},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nAaron Simumba. 2017. “What’s in the Numbers: A Look at the Lusaka\nSecurities Exchange.” April 6, 2017. https://asimumba.rbind.io//blog/luse."
  },
  {
    "objectID": "blog/mapping-twitter/index.html",
    "href": "blog/mapping-twitter/index.html",
    "title": "Mapping the Location of Twitter Followers and Following in R",
    "section": "",
    "text": "It is always nice to capture how wide and dense your twitter interactions spread. More so, the spread and locations of your twitter followers and those you are following. Making use of the twitteR package, and the inspiration from a post written by Jeff Leek, I attempt to map my followers’ locations and those I follow on twitter.\nJeff Leek wrote a twitterMap()function that lets you map the locations of your twitter followers and those you follow, the script that holds the function can be downloaded here.\nSince the data has to be mined from twitter to R. The first issue to address is setting up this link to twitter. Luckily, twitter’s Application Programming Interface (API) handles this integration. Twitter has a developer site, where you can set-up your app and acquire the authentication keys and tokens needed for R and twitter to communicate. For set-up instructions the following post has a nice step by step walk through set-up from the twitter side.\nFrom the R side. We need the following packages: ROAuth which is R’s interface to the OAuth (Open Authorization) - which is an open standard for token-based authentication and authorization on the Internet, allowing us give access to the third-party apps like twitter and Facebook to communicate with the local machine set-up.twitteR is the other package, plus the httr package.\nAuthenticating R to have access to your twitter account, the twitteR package has a function called setup_twitter_oauth(), where you are required to supply the keys and tokens to be found under the keys and tokens tab on your twitter developer profile. Supplying the consumer key and secret key obtained on-line, will prompt authorisation through the web browser, where a step by step prompt is given. Refer to the link above to the walk through set-up."
  },
  {
    "objectID": "blog/mapping-twitter/index.html#mapping-followers-and-following-locations",
    "href": "blog/mapping-twitter/index.html#mapping-followers-and-following-locations",
    "title": "Mapping the Location of Twitter Followers and Following in R",
    "section": "Mapping followers and Following locations",
    "text": "Mapping followers and Following locations\nJeff Leek provides the following description for the twitterMap function:\n\ntwitterMap <- function(userName,\n                       userLocation = NULL,\n                       fileName = \"twitterMap.pdf\",\n                       nMax = 1000,\n                       plotType = c(\"followers\",\n                                    \"both\",\n                                    \"following\")\n)\n\nwith arguments:\n\nuserName - the twitter username you want to plot\nuserLocation - an optional argument giving the location of the user, necessary when the location information you have provided Twitter isn’t sufficient for us to find latitude/longitude data\nfileName - the file where you want the plot to appear\nnMax - The maximum number of followers/following to get from Twitter, this is implemented to avoid rate limiting for people with large numbers of followers.\nplotType - if “both” both followers/following are plotted, etc.\n\nMapping my twitter data, I run the following code, supplying my twitter username, and changing the plotType argument to “both”, which plots both locations for my followers and those I follow.\nI call the twitterMap.R script through source(), the script loads the function in the work environment, in order to use the twitterMap() command.\n\nsource(\"twitterMap.R\")\n\ntwitterMap(\"zedsamurai\",\n           fileName = \"twitterMap.pdf\", \n           plotType = \"both\" \n           )\n\nThe following is the map of the locations of my twitter followers and those I follow.\n\n\n\nMap of twitter followers and followings\n\n\nSince most people and companies I follow and those that follow me are either investment firms, open source software developers and of course meme artists. Memes cannot miss out, they carry life.\nFor both the followers and those I follow, it can be seen that they are mainly located in North America, Europe and Africa. It is my hope to follow more of the budding tech and finance enthusiasts on the African continent. Next time I mine this data, it should paint a slightly more balanced picture."
  },
  {
    "objectID": "blog/mapping-twitter/twitter.html",
    "href": "blog/mapping-twitter/twitter.html",
    "title": "Let twitter do the job",
    "section": "",
    "text": "Although it is one thing to put your thoughts out. It is another issue to attract your intended audience. Thankfully, we are living in an era where more than ever, it is practically easier to reach a wider and remote audience in no time, with the emergence of social media.\nAfter finalising my previous blog post, I decided to tweet the post, with the hash tag #rstats. Which is like the defacto twitter label if you want to get noticed in the R community. It did not take a long time before the post was being liked and retweeted within the community. Next, I checked my Google analytics data and the traffic had gone tenfold, all because of one post that most found of interest.\nThe post was not really based on something original, I was implementing some work done by Jeff Leek. Which I am happy I got to learn and experience something new. Thanks to @Thinkr for initiating the retweet… As they say the rest is history.\n\n\n\nIn hindsight, I now believe more in the adage: “there is nothing new under the sun, whatever was built can be taken apart”. And the open source community have proven this many a times.\n\n\n\nCitationBibTeX citation:@online{simumba2017,\n  author = {Aaron Simumba},\n  title = {Let Twitter Do the Job},\n  date = {2017-10-06},\n  url = {https://asimumba.rbind.io//blog/mapping-twitter/twitter.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nAaron Simumba. 2017. “Let Twitter Do the Job.” October 6,\n2017. https://asimumba.rbind.io//blog/mapping-twitter/twitter.html."
  },
  {
    "objectID": "blog/metabase/index.html",
    "href": "blog/metabase/index.html",
    "title": "Deploying Metabase through Heroku App",
    "section": "",
    "text": "CitationBibTeX citation:@online{simumba2018,\n  author = {Aaron Simumba},\n  title = {Deploying {Metabase} Through {Heroku} {App}},\n  date = {2018-12-26},\n  url = {https://asimumba.rbind.io//blog/metabase},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nAaron Simumba. 2018. “Deploying Metabase Through Heroku\nApp.” December 26, 2018. https://asimumba.rbind.io//blog/metabase."
  },
  {
    "objectID": "blog/mobile-data/index.html",
    "href": "blog/mobile-data/index.html",
    "title": "Mobile and Internet Penetration in Zambia",
    "section": "",
    "text": "In this post, I take an Exploratory Data Analysis (EDA) of the mobile phone and internet user data statistics for Zambia. Making use of the Zambia Information and Communications, Technology Authority (ZICTA) data. I will walk through the data on the subscriber base, the user traffic and the cost differentials among the three mobile operators, these being: Airtel, Zamtel and MTN Zambia. In addition, internet usage and the operational data on Internet Service Providers(ISPs) will be used."
  },
  {
    "objectID": "blog/mobile-data/index.html#prerequisites",
    "href": "blog/mobile-data/index.html#prerequisites",
    "title": "Mobile and Internet Penetration in Zambia",
    "section": "Prerequisites",
    "text": "Prerequisites\nFor this analysis, I will mainly use the following packages. The beauty with the R language is that as a modular language, meaning it is fully extensible with add-on packages. I will make use of the ggplot2 for visualising the data; dplyr, which is the package made to seamlessly handle data transformations, will be used to tweak the data to match the objectives of the analysis. The other packages are readr for data import.\n\n# the packages can be installed running the following command\n# install.packges(\"ggplot2\", \"dplyr\", \"readr\", \"plotly\")\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(readr)\ntheme_set(theme_bw())"
  },
  {
    "objectID": "blog/mobile-data/index.html#data",
    "href": "blog/mobile-data/index.html#data",
    "title": "Mobile and Internet Penetration in Zambia",
    "section": "Data",
    "text": "Data\nThe data used in this analysis can be found on the ZICTA website. Since the data on the ZICTA website is not in a downloadable format. I had to turn to Microsoft Excel, with its option to mine the data from the web. The data on-line is in a format which is not very helpful to work with, so I had to clean up and reorder the data into a “tidy” format (each variable is a column and each observation occupying a row). The original data format can be seen in the screenshot below.\n\n\n\nSource: ZICTA\n\n\nRunning the following codes loads the data into the work environment, so we can work with it to do our analysis.\n\noperator_stats <- read_csv(\"operator_stats.csv\")\n\nisps <- read_csv(\"isps.csv\")\n\nnetwork_coverage <- read_csv(\"network_coverage.csv\")\n\noperator_stats <- operator_stats %>%\n  as_tibble()\n\nisps <- isps %>%\n  as_tibble()\n\nnetwork_coverage <- network_coverage %>%\n  as_tibble()\n\nThe head() command gives a view of the first 6 rows across variables of your data by default. This allows a quick peek at your data, revealing the data types and forms; and learn how the data is laid out .\n\nhead(operator_stats, 4)\nhead(isps, 4)\nhead(network_coverage, 4)\n\nThe names() function helps to know the variables or column names in the data. This comes in hand to pick the names and variables in the analysis.\n\nnames(operator_stats) # column names \n\nIt is important to learn the data types and the form your data assumes in your dataset. To achieve this, the str() function can be called. Since the the dataset has variables I am not interested in, I narrow the field with the select() from the dplyr package to select only variables of interest\n\noperator_stats %>%\n  \n  select(\n    Number_of_PSTN_Operators,\n    \n    Number_of_Mobile_Operators,\n    \n    Number_of_Registered_SIM_Cards,\n    \n    Minutes_of_use_Monthly\n  ) %>%\n  str()"
  },
  {
    "objectID": "blog/mobile-data/index.html#second-quarter-statistics-from-zicta",
    "href": "blog/mobile-data/index.html#second-quarter-statistics-from-zicta",
    "title": "Mobile and Internet Penetration in Zambia",
    "section": "Second quarter statistics from ZICTA",
    "text": "Second quarter statistics from ZICTA\nZICTA provides the following statistics on ICT as of the second quarter of 2017, as seen below. From these statistics, it can be seen that mobile subscription is slightly under 12.5 million subscribers, representing a penetration rate of 75.77%. On the internet side, a penetration rate is more than a one third standing at 35.88%.\n\n\n\nSource: ZICTA"
  },
  {
    "objectID": "blog/mobile-data/index.html#number-of-active-users",
    "href": "blog/mobile-data/index.html#number-of-active-users",
    "title": "Mobile and Internet Penetration in Zambia",
    "section": "Number of active users",
    "text": "Number of active users\nAs can be seen from the trend that spans from 2010 to the year 2017 up to Q2 statistics, there has been a relatively upward trend in active mobile user subscribers. With the period between 2010 to 2012 experiencing the biggest leap of nearly 50% change.\n\noperator_stats %>%\n  ggplot() +\n  geom_line(mapping = aes(x = Year,\n                          y = Number_of_active_Subscribers)) +\n  geom_point(mapping = aes(x = Year,\n                           y = Number_of_active_Subscribers)) +\n  labs(x = \"Year\",\n       y = \"Number of Active subscirbers\")  +\n  ggtitle(\"Trend of Active mobile subsribers\",\n          subtitle = \"Over the period from 2010 - 2017, Q1 & Q2\") \n\n\n\n\nThe plot below shows a positive correlation between the active mobile subscribers with the population estimate from the Central Statistics Office.\n\noperator_stats %>%\n  ggplot(aes(x = Population_CSO_Estimate,\n             y = Number_of_active_Subscribers)) +\n  geom_line() +\n  geom_point() +\n  theme(legend.title = element_text(size = 10)) +\n  \n  labs(x = \"Population Estimate\",\n       y = \"Number of Active subscirbers\") +\n  ggtitle(\"Population Vs. Number of Mobile Active subscribers\",\n          subtitle = \"Over the period from 2010 - 2017, Q1 & Q2\")"
  },
  {
    "objectID": "blog/mobile-data/index.html#change-in-the-active-user-base",
    "href": "blog/mobile-data/index.html#change-in-the-active-user-base",
    "title": "Mobile and Internet Penetration in Zambia",
    "section": "Change in the active user base",
    "text": "Change in the active user base\nFor much of the period under review, the number of active users was had a positive upward trend. Except for the years 2013 to 2014 and 2017 first quarter that showed a decline. Perhaps much of this decline for the 2013-2014, can be attributed to the fears the general populace had over compulsory sim registration. With fears spreading that the government was implementing this programme so they can spy on the citizens. The relevant authority made sure these fears were addressed by sensitizing the general public through press releases.\nGiven this was a free of charge exercise, some opted to make money by charging for registration. Further casting a negative image on the exercise.\n\nper_change <- operator_stats %>%\n  \n  select(Year, Population_CSO_Estimate,\n         Number_of_active_Subscribers) %>%\n  mutate(\n    Percentage_change_pop = (Population_CSO_Estimate - lag(Population_CSO_Estimate)) /\n      lag(Population_CSO_Estimate) * 100,\n    Percentage_change = (\n      Number_of_active_Subscribers - lag(Number_of_active_Subscribers)\n    ) /\n      lag(Number_of_active_Subscribers) * 100\n  )\n\nnames(per_change) <- c(\"Year\",\n                       \"Population\",\n                       \"Active subscribers\",\n                       \"% change pop.\",\n                       \"% change\")\n\nknitr::kable(per_change,\n             booktabs = TRUE,\n             caption =\n               \"Change in Population Vs.\n              change in the Active subscribers\")\n\n\nChange in Population Vs. change in the Active subscribers\n\n\nYear\nPopulation\nActive subscribers\n% change pop.\n% change\n\n\n\n\n2010\n13092666\n5447536\nNA\nNA\n\n\n2011\n13721498\n8164553\n4.802933\n49.8760724\n\n\n2012\n14156468\n10524676\n3.169989\n28.9069469\n\n\n2013\n14605555\n10395801\n3.172310\n-1.2245033\n\n\n2014\n15068729\n10114867\n3.171218\n-2.7023795\n\n\n2015\n15545778\n11557725\n3.165821\n14.2647254\n\n\n2016\n16037474\n12017034\n3.162891\n3.9740433\n\n\n2017\n16405229\n11916871\n2.293098\n-0.8335085\n\n\n2017\n16405229\n12429675\n0.000000\n4.3031766\n\n\n\n\n\nIn terms of the summary statistics, the active users numbers have average 10,285,415 a year. Considering a population size of nearly 16 million, this is a decent uptake in active mobile users.\n\nsummary(operator_stats$Number_of_active_Subscribers)\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n 5447536 10114867 10524676 10285415 11916871 12429675 \n\n\nFor the mobile penetration numbers per 100 inhabitants, the graph below shows an upward trend for much of the years under review. More in line with the active user base numbers.\n\noperator_stats %>%\n  \n  ggplot(aes(x = Year, y = Mobile_Penetration_100_Inhabitants)) +\n  geom_point() +\n  geom_line() +\n  labs(x = \"Year\",\n       y = \"Mobile Penetration per 100 Inhabitants\") +\n  \n  ggtitle(\"Mobile Penetration per 100 Inhabitants\",\n          subtitle = \"Over the period from 2010 - 2017, Q1 & Q2\")"
  },
  {
    "objectID": "blog/mobile-data/index.html#domestic-calls-traffic---incoming-and-outgoing",
    "href": "blog/mobile-data/index.html#domestic-calls-traffic---incoming-and-outgoing",
    "title": "Mobile and Internet Penetration in Zambia",
    "section": "Domestic calls Traffic - Incoming and Outgoing",
    "text": "Domestic calls Traffic - Incoming and Outgoing\n\noperator_stats %>%\n  \n  ggplot() +\n  geom_line(aes(x = Year,\n                y = Traffic_domestic_Incoming_Minutes_MNO_PSTN)) +\n  geom_line(aes(x = Year,\n                y = Traffic_domestic_Outgoing_Minutes_MNO_PSTN)) +\n  labs(x = \"Year\",\n       y = \"Domestic incoming/outgoing traffic in minutes\") +\n  ggtitle(\"Domestic incoming Vs. Outgoing Traffic in minutes\")"
  },
  {
    "objectID": "blog/mobile-data/index.html#international-incoming-and-outgoing-traffic",
    "href": "blog/mobile-data/index.html#international-incoming-and-outgoing-traffic",
    "title": "Mobile and Internet Penetration in Zambia",
    "section": "International incoming and outgoing traffic",
    "text": "International incoming and outgoing traffic\n\noperator_stats %>%\n  ggplot() +\n  geom_line(aes(x = Year,\n                y = Traffic_International_Incoming_Minutes_MNO_PSTN)) +\n  geom_line(aes(x = Year,\n                y = Traffic_International_Outgoing_Minutes_MNO_PSTN)) +\n  labs(x = \"Year\",\n       \n       y = \"International incoming/outgoing traffic in minutes\") +\n  \n  ggtitle(\"International incoming Vs. Outgoing Traffic in minutes\")"
  },
  {
    "objectID": "blog/mobile-data/index.html#network-coverage",
    "href": "blog/mobile-data/index.html#network-coverage",
    "title": "Mobile and Internet Penetration in Zambia",
    "section": "Network Coverage",
    "text": "Network Coverage\n\nnetwork_cover <-  network_coverage %>%\n  \n  select(Year,\n         Airtel_Zambia,\n         MTN_Zambia,\n         Zamtel)\n\nknitr::kable(network_cover, booktabs = TRUE,\n             caption = \"Percentage network coverage by operator\")  \n\n\nPercentage network coverage by operator\n\n\nYear\nAirtel_Zambia\nMTN_Zambia\nZamtel\n\n\n\n\n2010\nNA\nNA\nNA\n\n\n2011\nNA\n36.60%\n75.00%\n\n\n2012\nNA\n37.50%\n75.00%\n\n\n2013\n42.70%\n39.40%\n29.70%\n\n\n2014\n42.70%\n31.70%\n27.00%\n\n\n2015\n42.70%\n45.40%\n27.00%\n\n\n2016\n42.70%\n44.10%\n27.00%\n\n\n2017\n42.70%\n44.10%\n27.00%\n\n\n2017\n42.70%\n44.10%\n27.00%"
  },
  {
    "objectID": "blog/mobile-data/index.html#internet-penetration",
    "href": "blog/mobile-data/index.html#internet-penetration",
    "title": "Mobile and Internet Penetration in Zambia",
    "section": "Internet Penetration",
    "text": "Internet Penetration\nBroadband users\nThe broadband data which covers all those on Direct Subscriber Line,cable and optic fibre internet access, on average there are 3855730 users, with 75% of the users amounting to 5338910.\n\nsummary(operator_stats$Mobile_Broadband_users)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n 379888 2289147 4403005 3855730 5338910 6090412       1 \n\n\n\noperator_stats %>%\n  \n  ggplot() +\n  geom_line(aes(x = Mobile_Broadband_penetration_100_users,\n                y = Mobile_Broadband_users)) +\n  labs(x = \"Mobile broadband penetration per 100 users\",\n       y = \"Mobile broadband users\") +\n  \n  ggtitle(\n    \"Mobile broadband users Vs.\n          Mobile broadband penetration per 100 users\",\n    subtitle = \"Over the period from 2010 -\n          2017, Q1 & Q2\"\n  )"
  },
  {
    "objectID": "blog/mobile-data/index.html#internet-subscribers-vs-penetration-per-100-inhabitants",
    "href": "blog/mobile-data/index.html#internet-subscribers-vs-penetration-per-100-inhabitants",
    "title": "Mobile and Internet Penetration in Zambia",
    "section": "Internet subscribers Vs Penetration per 100 inhabitants",
    "text": "Internet subscribers Vs Penetration per 100 inhabitants\n\nisps %>%\n  ggplot() +\n  geom_line(aes(x = Fixed_internet_Penetration_per_100,\n                y = Number_of_Subscribers)) +\n  labs(x = \"Fixed internet penetration per 100 inhabitants\",\n       y = \"Number of subscribers\") +\n  \n  ggtitle(\"Fixed internet penetration per 100 inhabitants Vs.\n           Number of subscribers\",\n          subtitle = \"Over the period from 2010 - 2017, Q1 & Q2\")"
  },
  {
    "objectID": "blog/mobile-data/index.html#cost-differentials",
    "href": "blog/mobile-data/index.html#cost-differentials",
    "title": "Mobile and Internet Penetration in Zambia",
    "section": "Cost differentials",
    "text": "Cost differentials\nThe average cost for SMSs had maintained a very stable flow from the year 2010 to the year 2014, with on network SMS cost standing at K 0.30. Off net SMS cost averaged K0.24, significantly low than off net cost. A signal to growing competition and the mobile operators’ desire to benefit across networks. For the mobile phone tariff, for much of the period under review, we can see a relatively declining average cost of mobile tariff across different mobile operators. Although the year 2017 second quarter numbers show an increase in the tariff.The year 2017 saw a further increase in Excise duty taxes on telephone airtime from 15% to 15.5% for the 2017 budget, this is in addition to the 2015 increase from 10% to 15%. This can be reflected in the up-tick in the tariff costs beyond 2016.\n\noperator_stats %>%\n  \n  ggplot() +\n  \n  geom_line(aes(x = Year,\n                y = Voice_Tariff_Mobile)) +\n  annotate(\n    \"text\",\n    x = 2013,\n    y = 0.55,\n    label = \"Mobile voice tariff\",\n    size = 3\n  ) +\n  \n  geom_line(aes(x = Year,\n                y = SMS_on_net_Mobile)) +\n  annotate(\n    \"text\",\n    x = 2013,\n    y = 0.32,\n    label = \"SMS on-net tariff\",\n    size = 3\n  ) +\n  \n  geom_line(aes(x = Year,\n                y = SMS_off_net_Mobile)) +\n  \n  annotate(\n    \"text\",\n    x = 2013,\n    y = 0.25,\n    label = \"SMS off-net tariff\",\n    size = 3\n  ) +\n  labs(x = \"year\",\n       y = \"Mobile Voice tariff/ SMS - on and off net \") +\n  \n  ggtitle(\"Mobile voice tariff and SMS tariff - On & Off net\",\n          subtitle = \"Over the period from 2010 - 2017, Q1 & Q2\")"
  },
  {
    "objectID": "blog/mobile-data/index.html#mobile-voice-call-cost-perminute",
    "href": "blog/mobile-data/index.html#mobile-voice-call-cost-perminute",
    "title": "Mobile and Internet Penetration in Zambia",
    "section": "Mobile voice call cost per/minute",
    "text": "Mobile voice call cost per/minute\nOn all accounts, 2014 through to 2016 saw a significant drop in the cost of calling per minute. This is in comparison to the numbers prevailing from the year 2010 t0 2014. With Voice-off-net off peak on mobile maintaining a very steady cost per minute between 2010 to 2014. Conversely, Voice-on-net off peak on mobile had a relatively up-ward swing in the cost of calling per minute. For the year 2014, there was a drop in all forms of calling, whether on net and off network. This was preceded by the sim registration exercise, which has significant back drop on the mobile operators. The drop in cost may have been a means to lure back customers on their platform.\n\noperator_stats %>%\n  ggplot() +\n  geom_line(aes(x = Year,\n                y = Voice_on_net_peak_Mobile)) +\n  annotate(\n    \"text\",\n    x = 2013,\n    y = 1.18,\n    label = \"Voice on net peak - Mobile\",\n    size = 3\n  ) +\n  \n  geom_line(aes(x = Year,\n                y = Voice_on_net_off_peak_Mobile)) +\n  annotate(\n    \"text\",\n    x = 2013,\n    y = 0.9,\n    label = \"Voice on net off peak - Mobile\",\n    size = 3\n  ) +\n  \n  geom_line(aes(x = Year,\n                y = Voice_off_net_peak_Mobile)) +\n  annotate(\n    \"text\",\n    x = 2013,\n    y = 1.4,\n    label = \"Voice off net peak - Mobile\",\n    size = 3\n  ) +\n  \n  geom_line(aes(x = Year,\n                y = Voice_off_net_off_peak_Mobile)) +\n  annotate(\n    \"text\",\n    x = 2013,\n    y = 1.22,\n    label = \"Voice off net off peak - Mobile\",\n    size = 3\n  ) +\n  \n  labs(x = \"Year\",\n       y = \"Voice cost (Kwacha) per min\") +\n  \n  ggtitle(\"Voice cost in Kwachas per minute on mobile\")"
  },
  {
    "objectID": "blog/netflify-conflicts/index.html",
    "href": "blog/netflify-conflicts/index.html",
    "title": "How to resolve hugo version conflicts with the default version on netlify",
    "section": "",
    "text": "After all, the solution is quite simple and straight forward to implement. netlify uses version 0.17 as the default hugo build for deploying sites built under the hugo platform. And some hugo themes uses the more recent versions which are above version 0.17.\nA quick work around is to define the build environment with the version of hugo you desire or that fits with the theme being used. Under the site settings, you navigating to the build & deploy panel, locate the build environment variables. In the key value slot you type HUGO_VERSION and in the value place-holder, you define the hugo version. For instance version 0.27 as can be seen in the screenshot below. With this in place, you can expect a smooth build of the site and deployment.\n Happy blogging…\n\n\n\nCitationBibTeX citation:@online{simumba2017,\n  author = {Aaron Simumba},\n  title = {How to Resolve Hugo Version Conflicts with the Default\n    Version on Netlify},\n  date = {2017-09-13},\n  url = {https://asimumba.rbind.io//blog/netflify-conflicts},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nAaron Simumba. 2017. “How to Resolve Hugo Version Conflicts with\nthe Default Version on Netlify.” September 13, 2017. https://asimumba.rbind.io//blog/netflify-conflicts."
  },
  {
    "objectID": "blog/one-year/index.html",
    "href": "blog/one-year/index.html",
    "title": "One Year Anniversary",
    "section": "",
    "text": "The 26 August 2017 marks exactly one year since embarking on learning the R programming language. A year ago I had neither the foresight nor will strength of believing I would b this engrossed in the new found love for programming. But it didn’t take long before I found myself spending at least an hour a day playing with different skills and knowledge I had acquired. Coming from the background of playing with VBA in Excel. I got hooked every time I discovered I could do something more efficiently and, less painful in R than other data analysis packages I had used.\nAlthough R has a steep learning curve, its syntax is quite intuitive, and once you master the underlying language specific issues - such as, how it deals with different data types and some of the strengths at the core of the language; like its vectorised operations. It becomes quite easy to write code for relatively easy operations quickly and in a few lines of code."
  },
  {
    "objectID": "blog/one-year/index.html#what-have-i-learned-in-the-last-year",
    "href": "blog/one-year/index.html#what-have-i-learned-in-the-last-year",
    "title": "One Year Anniversary",
    "section": "What have I learned in the last year?",
    "text": "What have I learned in the last year?\nIn terms of learning, it’s been a decent year considering I have had to juggle between my other commitments and sparing time to learn and practice. Thankfully, the R community has grown both in size and commitment to developing new tools that not only make it easier to solve data problems but also makes it easy to report your findings.\nWithin the R ecosystem so many tools have been built,for me the most outstanding are the following: Thanks to Yihui Xie, his package, blogdown allows one to build a website making use of the static site generator hugo. The official package documentation can be found here. This website is a product of this package, built with a cactus theme. In this brief post I have given some details on how I built my website.\nThe second package from Yihui Xie I use often is bookdown. A great package for authoring books,articles and just able everything you can imagine. Both technical and non-technical documents. I beat myself whenever I recall the trauma I had to go through writing my undergraduate thesis with SPSS as a tool of choice for analysis. Picture this, I would have to manually copy my graphs and analyses into Word.Then, after I realise I have a mistake in one of the figures, you can probably guess the next step, going back step by step through the painful process. Thanks to bookdown, which implements Rmarkdown, all it takes is writing the prose with code embedded in the same plain text document. With the use of pandoc, it allows you to convert to various output formats such as HTML, Word, and PDF. You can check the package documentation and the accompanying book.. I have managed to author two full technical documents and many more brief articles."
  },
  {
    "objectID": "blog/one-year/index.html#why-i-started-in-the-first-place",
    "href": "blog/one-year/index.html#why-i-started-in-the-first-place",
    "title": "One Year Anniversary",
    "section": "Why I started in the first place",
    "text": "Why I started in the first place\nSince I started R, mainly just to learn enough to be able to do simple data analyses, I later realised, there is a more interesting field beyond mere simple analyses. I started taking both paid and free on-line courses in Data Science. DataCamp and Coursera are the two paid platforms I am using this far. But of the two, Coursera stands out for me. Their model incorporates top universities as partners and financial aid is available for those that can’t afford to pay. Of course, with the usual scholarship questions to be answered. I’m happy to have received my first financial aid to pursue a course in probability and data offered by Duke University. But the learning doesn’t end there, Hadley Wickham and Garret Grolemund of RStudio have a free on-line book on R for Data Science. This book implements a collection of R packages under the tidyverse banner, developed to make working with data less painful and more smoother- plus it contains many real world data science problems.\nIn hindsight, learning how to program with R is one of the best decisions I made. I have been exposed to a lot and learnt many things, which 12 months ago was all Greek mythology.\nI can only imagine what is in store in the next 12 months.\nIn closing, I have realised git and github as tools for version control mean life to code crunchers…"
  },
  {
    "objectID": "blog/oss/index.html",
    "href": "blog/oss/index.html",
    "title": "Money is everything, but you haven’t met Open Source Software maintainers",
    "section": "",
    "text": "Conversely, for a few months now, I have been using many Open Source Software. Oh boy!, it is amazing to see and interact with many people that are involved in these projects, they treat and welcome you whole heartedly for nothing but a “thank you”. Open source software developers have to be some of the nicest people I have come across.\nIt is natural to receive attention when it is financially beneficial to the giver and receiver. But for OSS developers, they seem to take pride in simply being there to help make people’s lives to be better. The painstakingly patience is astonishing. Not even the most stupid questions seem to bother many of them.\nIf there is something I have learned, is, if you are passionate about something, eventually you will dedicate you time to make it come to fruition, even at the expense of your own free time. This is my an unsolicited “thank you” to all the OSS developers.\n\n\n\nCitationBibTeX citation:@online{simumba2018,\n  author = {Aaron Simumba},\n  title = {Money Is Everything, but You Haven’t Met {Open} {Source}\n    {Software} Maintainers},\n  date = {2018-02-24},\n  url = {https://asimumba.rbind.io//blog/oss},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nAaron Simumba. 2018. “Money Is Everything, but You Haven’t Met\nOpen Source Software Maintainers.” February 24, 2018. https://asimumba.rbind.io//blog/oss."
  },
  {
    "objectID": "blog/pdf-data/index.html",
    "href": "blog/pdf-data/index.html",
    "title": "Breathing life back into PDF presented Data",
    "section": "",
    "text": "It is almost not surprising to find most of the summarised data is presented in the form of a report - whose format is mainly Portable Document Format (PDF). The challenge is when you would like to access that data in a dynamic format and form - where it can be analysed, reformatted and reshaped to your desire; a requirement which is hard, if not impossible to achieve with data presented in a PDF report. Trying to do so would be like wishing to extract water from a rock, which is an endeavour in futility.\nThe good news is, technology seem to run on a brain of its own. While one side of the technology spectrum impedes, another end liberate. One such solution to extracting the dead and static PDF presented data, is to turn to the powerful and versatile R package, tabulizer. The tabulizer package is an R wrapper for the powerful PDF extractor Java library Tabula. This package allows one to extract with ease, data presented in tables in a PDF document. For as long as the data is in a clean and uncluttered format. The extract_tables() function will try to guess the delimiters for the data and extract the data in the format which maintains close to the original data outline."
  },
  {
    "objectID": "blog/pdf-data/index.html#installation",
    "href": "blog/pdf-data/index.html#installation",
    "title": "Breathing life back into PDF presented Data",
    "section": "Installation",
    "text": "Installation\nFor the installation and usage, the package depends on Java. The appropriate Java Development Kit can be downloaded straight from the Oracle website here. Installation instructions are platform specific. Follow the instructions depending on your OS. I am on Windows, so I installed Java, running the jdk-8u144-windows-x64.exe executable file.\nInstalling tabulizer package, this can be installed from github. There is only the development version of the package, you will not find it on CRAN.\n\nif(!require(\"ghit\")){\n                       install.packages(\"ghit\")\n}\n# on 64-bit Windows\nghit::install_github(c(\"leeper/tabulizerjars\", \n                       \"leeper/tabulizer\"), \n                         INSTALL_opts = \"--no-multiarch\"\n                     )\n# elsewhere\nghit::install_github(c(\"leeper/tabulizerjars\", \n                       \"leeper/tabulizer\"))\n\nThis will download and install other Java related packages tabulizer depends on."
  },
  {
    "objectID": "blog/pdf-data/index.html#demo",
    "href": "blog/pdf-data/index.html#demo",
    "title": "Breathing life back into PDF presented Data",
    "section": "Demo",
    "text": "Demo\nFor demonstration purpose, I will use the report from the Central Statistics Office (CSO), Zambia, on Zambia Census Projection 2011-2035. Below is the outline of the sample data as presented in the PDF report.\n\n\n\nSample Data File - Source: CSO\n\n\nWe call the tabulizer package with the following command.\n\nlibrary(\"tabulizer\")\n\nThe main function is the extract_tables(). The first argument is the PDF file or report where the targeted table(s) is/are. The second argument is the pages, where you specify the page number the table of data is. There are other arguments such as area, which you can specify the targeted area(s) to extract. columns which matches with the number of pages to be extracted. This argument allows for each page extracted to be stored in its own separate column.The guess argument, which by default is =TRUE, allows for the function to guess the location of the table(s) on each page. For a list of all the arguments: run ?extract_tables in the R console. By default, the data is extracted as a list. Lists in R can be thought of as a vector containing other objects. We can zoom in on a particular object using the double square brackets,[[]]. For instance, the first object in the variable is indexed by the number 1, and the second object by 2, and so on. Since,only one table is being extracted, the variable below contain one column; extracted with this command,cso_table[[1]].\nThe default way, extract_table() extracts the data as a list of character matrices. This helps in cases where the data is irregular and cannot be properly coerced to a data frame (row by column format). To change this behaviour so that the extracted data is coerced to a data frame, we supply the method argument, and have data.frame as the value.\n\ncso <- (\"https://goo.gl/d2xMwS\")\n# This is the shortened version of the original URL.\n\ncso_table <- extract_tables(cso, pages = 24,\n                            method = \"data.frame\")\n# We are going to pass the cso variable to the extract_tables() function\ncso_column <- cso_table[[1]]\n\n# The table of interest is on page 24, the other arguments are left as defaults\n\nFrom the extracted results, it can be seen the output is not in a “tidy” format, to allow any meaning analyses to be done. The next step would be reshaping and reordering the extracted results into a neat data frame."
  },
  {
    "objectID": "blog/pdf-data/index.html#tidying-the-data",
    "href": "blog/pdf-data/index.html#tidying-the-data",
    "title": "Breathing life back into PDF presented Data",
    "section": "Tidying the data",
    "text": "Tidying the data\nTwo approaches can be implemented here: the easy way or the hard way.\n\nFirstly, the easy way. We can write the data to a CSV file and clean the data in Microsoft Excel. The solution is to use the write.csv() function. The first argument in the function is the data object. The file argument, you define the output file name together with the file extension - in our case it is a .CSV extension. The row.names specifies whether to include the default index R attaches to the data, which spans the length of your data.\n\n\n# I have passed a relative path where I want the CSV file to be stored\nwrite.csv(cso_column, file = \"cso_data.csv\",\n          row.names = FALSE) \n\nAfter cleaning the data in Excel, it can be re-imported to aid in analysis.\n\nSecond choice, the hard way. The tidyverse package has a suite of packages built specifically to handle such tasks. Thedplyr package, is one such package, which represents the grammar of data manipulation. Using well crafted verbs, one can transform, order, filter etc.. data with ease."
  },
  {
    "objectID": "blog/pdf-data/index.html#welcome-to-the-tidyverse",
    "href": "blog/pdf-data/index.html#welcome-to-the-tidyverse",
    "title": "Breathing life back into PDF presented Data",
    "section": "Welcome to the tidyverse",
    "text": "Welcome to the tidyverse\nFirst step is to clean the data, eliminating unwanted variables and title headers. That is in addition to transforming the data into a “tidy” format - A variable per column, observation per row, and a value per cell. The command below eliminates the first, second, and the last three row of the extracted data.\ntidyr package is used to gather the observations in the columns into rows and combine all the observations across 2 columns. The function gather() achieves this in the tidyr package.\nAfter gathering the data from the columns to rows, the second issue is to index the numbers by the corresponding provinces. This is achieved by replicating the provinces to span the length of the numbers. Combining the row names with their corresponding numbers completes our simple data extraction exercise.\n\ncso_data <- cso_data %>%\n  as_tibble()\n\ncso_provincial <- cso_data %>%\n  filter(sex == \"Total\") %>%\n  select(`2011`:`2035`) %>%\n  gather(key = \"year\", value = \"census_proj\")\n\nprovince <- rep(\n  c(\n    \"central\",\n    \"copperbelt\",\n    \"eastern\",\n    \"luapula\",\n    \"lusaka\",\n    \"muchinga\",\n    \"northern\",\n    \"north.western\",\n    \"southern\",\n    \"western\"\n  )\n  ,\n  6\n)\n\ncso_transformed <- cbind(cso_provincial, province) %>%\n  select(year, province, census_proj) %>%\n  as_tibble()\n\ncso_transformed\n\n# A tibble: 60 × 3\n   year  province      census_proj\n   <chr> <chr>         <chr>      \n 1 2011  central       1,355,775  \n 2 2011  copperbelt    2,143,413  \n 3 2011  eastern       1,628,880  \n 4 2011  luapula       1,015,629  \n 5 2011  lusaka        2,362,967  \n 6 2011  muchinga      749,449    \n 7 2011  northern      1,146,392  \n 8 2011  north.western 746,982    \n 9 2011  southern      1,642,757  \n10 2011  western       926,478    \n# … with 50 more rows\n\n\nFor the full data table view, see the table below.\n\nknitr::kable(cso_transformed, booktabs = TRUE,\n             caption = \"Census data per Province\")  \n\n\nCensus data per Province\n\n\nyear\nprovince\ncensus_proj\n\n\n\n\n2011\ncentral\n1,355,775\n\n\n2011\ncopperbelt\n2,143,413\n\n\n2011\neastern\n1,628,880\n\n\n2011\nluapula\n1,015,629\n\n\n2011\nlusaka\n2,362,967\n\n\n2011\nmuchinga\n749,449\n\n\n2011\nnorthern\n1,146,392\n\n\n2011\nnorth.western\n746,982\n\n\n2011\nsouthern\n1,642,757\n\n\n2011\nwestern\n926,478\n\n\n2015\ncentral\n1515086\n\n\n2015\ncopperbelt\n2362207\n\n\n2015\neastern\n1813445\n\n\n2015\nluapula\n1127453\n\n\n2015\nlusaka\n2777439\n\n\n2015\nmuchinga\n895058\n\n\n2015\nnorthern\n1304435\n\n\n2015\nnorth.western\n833818\n\n\n2015\nsouthern\n1853464\n\n\n2015\nwestern\n991500\n\n\n2020\ncentral\n1734601\n\n\n2020\ncopperbelt\n2669635\n\n\n2020\neastern\n2065590\n\n\n2020\nluapula\n1276608\n\n\n2020\nlusaka\n3360183\n\n\n2020\nmuchinga\n1095535\n\n\n2020\nnorthern\n1520004\n\n\n2020\nnorth.western\n950789\n\n\n2020\nsouthern\n2135794\n\n\n2020\nwestern\n1076683\n\n\n2025\ncentral\n1979202\n\n\n2025\ncopperbelt\n3016344\n\n\n2025\neastern\n2344980\n\n\n2025\nluapula\n1439877\n\n\n2025\nlusaka\n4004276\n\n\n2025\nmuchinga\n1326222\n\n\n2025\nnorthern\n1763638\n\n\n2025\nnorth.western\n1080072\n\n\n2025\nsouthern\n2445929\n\n\n2025\nwestern\n1173598\n\n\n2030\ncentral\n2254435\n\n\n2030\ncopperbelt\n3402007\n\n\n2030\neastern\n2655422\n\n\n2030\nluapula\n1623991\n\n\n2030\nlusaka\n4704135\n\n\n2030\nmuchinga\n1587414\n\n\n2030\nnorthern\n2040926\n\n\n2030\nnorth.western\n1227481\n\n\n2030\nsouthern\n2793523\n\n\n2030\nwestern\n1286880\n\n\n2035\ncentral\n2565450\n\n\n2035\ncopperbelt\n3823642\n\n\n2035\neastern\n3001152\n\n\n2035\nluapula\n1834667\n\n\n2035\nlusaka\n5465775\n\n\n2035\nmuchinga\n1879642\n\n\n2035\nnorthern\n2355007\n\n\n2035\nnorth.western\n1397137\n\n\n2035\nsouthern\n3184855\n\n\n2035\nwestern\n1416331\n\n\n\n\n\nWe can finally take a breather, and enjoy!\n\n\n\n\nvia GIPHY"
  },
  {
    "objectID": "blog/stock-returns/index.html",
    "href": "blog/stock-returns/index.html",
    "title": "Stocks Portfolio Analysis",
    "section": "",
    "text": "library(quantmod)\nlibrary(ggplot2) \nlibrary(xts)\nlibrary(highcharter)\nlibrary(PerformanceAnalytics)\nlibrary(dygraphs)\n\n\n\n\n\n## Dates have been loaded as a factor instead of as dates. \n#There is need to convert this variable to the date class and covert the rest of the variables to xts(Extensible Time Series)\n\ntemp <- xts(x =fundreturn[,-1], order.by = as.Date(fundreturn[,1])) \n# all the variables except data are coerced to xts using date as the index.\n# Overwriting the fundreturn object with the xts temp object\nfundreturn <- temp\nstr(fundreturn)\n\nAn 'xts' object on 2007-06-01/2016-11-01 containing:\n  Data: num [1:114, 1:18] 0.024 0.0057 0.0165 0.0126 -0.0301 0.0026 0.0045 0.0218 0.027 0.0315 ...\n - attr(*, \"dimnames\")=List of 2\n  ..$ : NULL\n  ..$ : chr [1:18] \"SPY\" \"IJS\" \"VTI\" \"IYR\" ...\n  Indexed by objects of class: [Date] TZ: UTC\n  xts Attributes:  \n NULL\n\nrm(temp) # getting rid of temp object\n## Next I, stack variables of the same unique class within the same column.\n#That is the returns of all the stocks in one column and the tickers in another column, indexed by the date variable.\n\n# make use of a temporary object.\ntemp <- data.frame(index(fundreturn), \n                   stack(as.data.frame(coredata(fundreturn))))\nfundreturn_final <- temp\n\n\n# Give the variables more descriptive names \n    names(fundreturn_final)[1] <-  \"Year\"\n    names(fundreturn_final)[2] <-  \"PercentageReturn\"\n    names(fundreturn_final)[3] <-  \"Stockticker\"\n    names(fundreturn_final)\n\n[1] \"Year\"             \"PercentageReturn\" \"Stockticker\"     \n\n  rm(temp) # removing the temp. object\n  # we coerce the data frame back to xts to be able to use quantmod and highcharter\n#fundreturn_final <- xts(x=fundreturn_final[-1], order.by = as.Date(fundreturn_final[,1]))\n\n\nggplot(data = fundreturn_final, aes(x=Year, \n                                    y=PercentageReturn, color=Stockticker)) +\n  geom_line()\n\n\n\n\n\n# Google stock Performance\n# GOOG\nGOOG <- subset(fundreturn_final, Stockticker==\"GOOG\")\n\ng <- ggplot(data = GOOG, aes(x=Year, y=PercentageReturn)) +\n  geom_line()\n# add features\ng <- g + ggtitle(\"Google Monthly Return\", \n                 subtitle = \"For the Period between June 2007 - Nov. 2016\") + \n  \n      theme(panel.background = element_rect(fill = \"white\",\n                                            colour = \"grey50\"),\n            axis.text = element_text(colour = \"blue\"),\n            \n            axis.title.y = element_text(size = rel(1.0), angle = 90),\n            axis.title.x = element_text(size = rel(1.0), angle = 360))\n\ng <- g + labs(x = \"Year\",\n        y =\"Return\") \n\ng + annotate(\"text\",x=as.Date(\"2009-09-01\"),\n             y=0.3245,label=\"HR\",\n             fontface=\"bold\",size=3, \n             colour = \"forestgreen\") +\nannotate(\"text\",x=as.Date(\"2010-04-01\"),\n         y=-0.1900,label=\"LR\",\n         fontface=\"bold\",size=3,\n         colour =\"red\") \n\n\n\n\n\n# Portfolio Performance Appraisal\n\n# Having the following portfolio\n\n# Google = GOOG, Amazon = AZMN,Apple = AAPL JP Morgans = JPM, Microsoft = MSFT, General Electric = GE, and Hewlett Packard = HPQ\n#, \"GE\", \"HPQ\"\n\np1 <- subset(fundreturn_final, Stockticker ==\"AMZN\")\np2 <- subset(fundreturn_final, Stockticker ==\"MSFT\")\np3 <- subset(fundreturn_final, Stockticker ==\"AAPL\")\np4 <- subset(fundreturn_final, Stockticker ==\"GOOG\")\n\nportfolio <- rbind(p1,p2,p3,p4) # binding the returns into one returns variable\nrm(\"p1\",\"p2\", \"p3\",\"p4\") # Removal of the temp. subsets \n \n# quick visual representation of the data\np <- ggplot(data = portfolio, aes(x = Year, y =PercentageReturn, colour = Stockticker))+geom_line()\np + labs(\n        x = \" Year\",\n        y = \"Return\",\n        colour = \"Stock ticker\") +\n    ggtitle(\" Apple, Amazon and Google Stock Returns\",subtitle =\" For the period June 2007 - Nov. 2016\")\n\n\n\n\n\n#Dygraphing\n\np1 <- subset(fundreturn_final, Stockticker ==\"AMZN\")\np2 <- subset(fundreturn_final, Stockticker ==\"MSFT\")\np3 <- subset(fundreturn_final, Stockticker ==\"AAPL\")\np4 <- subset(fundreturn_final, Stockticker ==\"GOOG\")\n\n\n# Converting to xts before graphing\nAMZN <- xts(x = p1[,c(-1,-3)], order.by = p1[,1])\nMSFT <- xts(x = p2[,c(-1,-3)], order.by = p2[,1])\nAAPL <- xts(x = p3[,c(-1,-3)], order.by = p3[,1])\nGOOG_ <- xts(x = p4[,c(-1,-3)], order.by = p4[,1])\n\nrm(\"p1\",\"p2\", \"p3\",\"p4\") # Removal of the temp. subsets \n\n\nmerged_returns <- merge.xts(AMZN,MSFT,AAPL,GOOG_) # merging the separate share returns into one xts object.\n\ndygraph(merged_returns, main = \"Amazon v Microsoft v Apple v Google\") %>% # Using pipes to connect the codes\n  dyAxis(\"y\", label =\"Return\") %>%\n  dyAxis(\"x\", label =\"Year\") %>%\n  dyOptions(colors = RColorBrewer::brewer.pal(4, \"Set2\")) \n\n\n\n\n\n\n# Let's now evaluate the portfolio\n\n## Part of the code is adapted from Jonathan Regenstein from RStudio\n\n# We assume an equally weighted portfolio. Allocating 25% to all the stocks in our portfolio\nw <- c(.25,.25,.25,.25)\n\n# We use the performanceAnalytics built infuction Return.porftolio to calculate portfolio monthly returns\n\nmonthly_P_return <-  Return.portfolio(R = merged_returns, weights = w)\n\n# Use dygraphs to chart the portfolio monthly returns.\ndygraph(monthly_P_return, main = \"Portfolio Monthly Return\") %>% \n  dyAxis(\"y\", label = \"Return\")\n\n\n\n\n\n\n# Add the wealth.index = TRUE argument and, instead of returning monthly returns,\n# the function will return the growth of $1 invested in the portfolio.\ndollar_growth <- Return.portfolio(merged_returns, weights = w, wealth.index = TRUE)\n\n# Use dygraphs to chart the growth of $1 in the portfolio.\ndygraph(dollar_growth, main = \"Growth of $1 Invested in Portfolio\") %>% \n  dyAxis(\"y\", label = \"$\")\n\n\n\n\n\n\n# Calculating the Sharpe Ratio\n# Taking the US 10 Year Treasury Rate of 2.40% as the risk free rate.\n# Making use of the built in SharpeRatio function in Performance Analytics package.\nprint(sharpe_ratio <- round(SharpeRatio(monthly_P_return, Rf = 0.024), 4))\n\n                                portfolio.returns\nStdDev Sharpe (Rf=2.4%, p=95%):           -0.0634\nVaR Sharpe (Rf=2.4%, p=95%):              -0.0424\nES Sharpe (Rf=2.4%, p=95%):               -0.0298\n\n\n\n\n\nCitationBibTeX citation:@online{simumba2017,\n  author = {Aaron Simumba},\n  title = {Stocks {Portfolio} {Analysis}},\n  date = {2017-03-17},\n  url = {https://asimumba.rbind.io//blog/stock-returns},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nAaron Simumba. 2017. “Stocks Portfolio Analysis.” March 17,\n2017. https://asimumba.rbind.io//blog/stock-returns."
  },
  {
    "objectID": "blog/tableau/index.html",
    "href": "blog/tableau/index.html",
    "title": "Demo: Embedding Tableau public Visualizations",
    "section": "",
    "text": "<iframe \nsrc=\"https://public.tableau.com/views/StockMarkets/LUSEKPI?:showVizHome=no&:embed=true\" width=\"652px\" height=\"756px\" style=\"border: 0px;\" scrolling=\"no\">\n</iframe>\nNOTE: You must strip off the extra text from the URL to the right side of the ? from the URL obtained from the Tableau public viz. There after add the colon followed by other arguments as indicated above.\nHere is the breakdown of each argument and value:\n\nsrc - This is the source URL from the tableau public visualisation dashboard.\nshowVizHome=no&:embed=true” - This allows the visualization to display on the external site its being embedded.\nwidth=“652px” height=“756px” - You can set the width to height ratio, expressed either as px or as %.\nstyle=“border: 0px;” - This eliminates the shadow border around the visualization.\nscrolling=“no” - setting this to no allows for the visualization to fully occupy the available space, and not have extended scrolling bars.\n\n\n\n\nThat is all. It is that simple to leverage Tableau public infrastructure for great visualizations which can be shared on private websites.\n\n\n\nCitationBibTeX citation:@online{simumba2017,\n  author = {Aaron Simumba},\n  title = {Demo: {Embedding} {Tableau} Public {Visualizations}},\n  date = {2017-11-12},\n  url = {https://asimumba.rbind.io//blog/tableau},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nAaron Simumba. 2017. “Demo: Embedding Tableau Public\nVisualizations.” November 12, 2017. https://asimumba.rbind.io//blog/tableau."
  },
  {
    "objectID": "blog/website/index.html",
    "href": "blog/website/index.html",
    "title": "Thoughts to Words - An Introduction to Aaron Simumba’s Website",
    "section": "",
    "text": "This post was originally submitted to the Rbind support website."
  },
  {
    "objectID": "blog/website/index.html#introduction",
    "href": "blog/website/index.html#introduction",
    "title": "Thoughts to Words - An Introduction to Aaron Simumba’s Website",
    "section": "Introduction",
    "text": "Introduction\nHi, my name is Aaron Simumba, and you can find my website on the following URL: https://asimumba.rbind.io/. And on Github, plus you can follow me on Twitter. A brief background about myself: I often like to refer to myself as a “Lost Accountant” - because professionally I trained as an accountant, who lost his way to practise accounting and ended up into the data science space. I was excited to be in finance and accounting, I loved all the glory that came with working with financial data. But at heart, I loved computers and statistics. Statistics was probably one of my favourite courses at university. If there is one thing I got out of the accounting degree was that: “Debits should equal credits and vice versa”; and this has been my motivation to try to always find a reconciliation point for all I do."
  },
  {
    "objectID": "blog/website/index.html#blogging-journey",
    "href": "blog/website/index.html#blogging-journey",
    "title": "Thoughts to Words - An Introduction to Aaron Simumba’s Website",
    "section": "Blogging journey",
    "text": "Blogging journey\nI have always had a knack to express my thoughts. Previously I blogged with blogger and Wordpress. Mostly I blogged about random stuff that today I am ashamed to re-read. But that provided a very good foundation for harnessing my interest in blogging."
  },
  {
    "objectID": "blog/website/index.html#getting-lost-in-the-wilderness",
    "href": "blog/website/index.html#getting-lost-in-the-wilderness",
    "title": "Thoughts to Words - An Introduction to Aaron Simumba’s Website",
    "section": "Getting lost in the wilderness",
    "text": "Getting lost in the wilderness\nDuring my final undergraduate year, I so looked forward to finishing and venture into other interesting life persuasions. But that was one half of finishing all the university business - the other half involved writing my final year thesis. I was happy to finally get on with challenging my thought process. Unfortunately this enthusiasm was short lived. SPSS was in my way! I hated the work process of analysis in SPSS, and then copy the output to Microsoft word. Oh lord! cant there be something better? At the end of the day I went on with this process until the bitter end.\nPost university, I started looking for a better analysis environment, luckily I stumbled on R and all the tools in its ecosystem. And as you can guess I have not looked back. Been using R and its tools for now 1 year 7 months. If I’m to describe the experience, I would say, it’s been fun and challenging. With plenty of promises for the future."
  },
  {
    "objectID": "blog/website/index.html#website",
    "href": "blog/website/index.html#website",
    "title": "Thoughts to Words - An Introduction to Aaron Simumba’s Website",
    "section": "Website",
    "text": "Website\nIf there is one thing hard to be satisfied with is the visual aesthetics of the website. I first created my blogdown powered website in 2017 right around July. But between then and now, I have changed a minimum four website themes. This was mainly due the fact I had zero knowledge of HTML, let alone CSS. I took each theme as it came and simply replaced template filled sections with my own details. This left a hole which needed to be filled. I wanted to have control over even the most miniature website aspect. Fast forward, I crashed HTML and CSS courses to try and learn enough skills to be dangerous, of course in a minimal way. I want to thank Yihui Xie, from whom I have learnt a lot. His blog has great material to both his tools and his take on other open source software. Of course don’t forget to read the blogdown book.\nMy website is built on the hugo, github, netlify and blogdown platform. I am using the Beautiful Hugo theme. I like this theme for the following reasons: natively it comes with support for syntax highlighting, mathjax, a big image cover for the landing page and photoswipe, embedding a gallery of images is something I wanted.\n\n\n\nscreenshot\n\n\nThe theme has quite a clean design, which you can get started with a new website in minutes. Minimalistic design if you decide to strip out the images. Looking at the amount of work I did to achieve the visual effects as seen below. I must say I’m pleased and happy to call this a personal website!\n\n\n\nwebsite\n\n\nI broke and repositioned a lot of the native features to achieve the design as seen above. I let go of the avatar container on the navigation bar, re-aligned the navbar links to the right from their original left hand position. Added font awesome icons to some links.\n\n\n\nnavbar\n\n\nFor the landing page, I moved posts preview to the blog tab. And in addition, adjusted the big image of the landing page to cover the whole page. Resized the footer to accommodate the big front image.\nI love the post preview from the blog section. It gives a brief section to preview the content before delving fully into the post. From the original theme structure, this is presented on the front page if you use /content/post/ structure. To avoid this, I changed it to /content/blog/. All my posts reside in the /blog subdirectory.\n\n\n\nblog"
  },
  {
    "objectID": "blog/website/index.html#end-goal",
    "href": "blog/website/index.html#end-goal",
    "title": "Thoughts to Words - An Introduction to Aaron Simumba’s Website",
    "section": "End goal",
    "text": "End goal\nGoing forward, I will use this website to document my R learning path. And occasionally anything I find remotely interesting for the future me for reference purposes. More like turning my wild thoughts to written engravings. Happy blogging…\nI cannot document all the small but significant changes I made. The best option would be to fork the site and have a look at the changes made. For the source to my website, you can find it here\nEdit: This is a follow-up to my previous post"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDec 26, 2018\n\n\nAaron Simumba\n\n\n0 min\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\ndatahack4fi\n\n\ntwitter\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nNov 11, 2018\n\n\nAaron Simumba\n\n\n0 min\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nStock market\n\n\n\n\n\n\n\n\n\n\n\nNov 7, 2018\n\n\nAaron Simumba\n\n\n0 min\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nopen sourse\n\n\n\n\n\n\n\n\n\n\n\nFeb 24, 2018\n\n\nAaron Simumba\n\n\n1 min\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nblogdown\n\n\nR\n\n\nhugo\n\n\n\n\n\n\n\n\n\n\n\nFeb 19, 2018\n\n\nAaron Simumba\n\n\n4 min\n\n\n\n\n\n\n  \n\n\n\n\n\nThrough Tableau public integration\n\n\n\n\n\n\n\n\n\n\nNov 12, 2017\n\n\nAaron Simumba\n\n\n1 min\n\n\n\n\n\n\n  \n\n\n\n\n\nImplementation through Tabulizer\n\n\n\n\nrmarkdown\n\n\n\n\n\n\n\n\n\n\n\nOct 12, 2017\n\n\nAaron Simumba\n\n\n6 min\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOct 6, 2017\n\n\nAaron Simumba\n\n\n1 min\n\n\n\n\n\n\n  \n\n\n\n\n\nTwitter data mining\n\n\n\n\n\n\n\n\n\nOct 2, 2017\n\n\nAaron Simumba\n\n\n4 min\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSep 27, 2017\n\n\nAaron Simumba\n\n\n12 min\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSep 13, 2017\n\n\nAaron Simumba\n\n\n1 min\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nreflections\n\n\n\n\n\n\n\n\n\n\n\nAug 25, 2017\n\n\nAaron Simumba\n\n\n3 min\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nhugo\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nJul 3, 2017\n\n\nAaron Simumba\n\n\n1 min\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nApr 6, 2017\n\n\nAaron Simumba\n\n\n6 min\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\ngoogle\n\n\napple\n\n\nmsft\n\n\n\n\n\n\n\n\n\n\n\nMar 17, 2017\n\n\nAaron Simumba\n\n\n4 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Get to know me",
    "section": "",
    "text": "I work in the financial services sector as a Business Intelligence & Analytics Lead. Previously, I worked for mPharma as a Data Management Associate.\nIn another life, I was an Accounting and Finance major. Graduated from the University of Namibia with a BA degree in Accounting(Honours). Equally tutored Business Administration students in the art of business and accounting at the University of Zambia.\nI am lost in the Data Science web… hoping to find my way out of the maze soon. I find data and the process of data analysis to glean insight from the raw data, overly exciting and fascinating alike.\nI’m equally fascinated by stock markets and everything with a business and finance buzz.\nI can be found on Github poking around interesting projects… and occasionally I rant on twitter.\nFor this website:\n\nThis will be a place where I share my daily musings in the world of R, analytics and mostly, everything remotely interesting. I have a knack for learning interesting data analytics technologies.\n\nLet’s have fun! 😁\nRecommended blogs\nYihui Xie\nDavid Robinson - Variance explained\nSimply Statistics\nR Bloggers\nRStudio blog\nRStudio community: Everything RStudio\nR Weekly"
  },
  {
    "objectID": "LICENSE.html",
    "href": "LICENSE.html",
    "title": "R . Data Science . Business Intelligence",
    "section": "",
    "text": "Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the “Software”), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\nThe above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\nTHE SOFTWARE IS PROVIDED “AS IS”, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\n\n\nCitationBibTeX citation:@online{untitled,\n  author = {},\n  url = {https://asimumba.rbind.io//LICENSE.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nn.d. https://asimumba.rbind.io//LICENSE.html."
  },
  {
    "objectID": "portfolio.html",
    "href": "portfolio.html",
    "title": "Portfolio",
    "section": "",
    "text": "Sector Visualisation for BongoHive"
  },
  {
    "objectID": "talks.html",
    "href": "talks.html",
    "title": "Talks",
    "section": "",
    "text": "2017 Facebook Developer Circles - AI Masterclass speech 101 panelist under the topic: AI, is the Zambian Industry Ready for the next wave in tech, and what are the opportunities.\n2021 Deep Learning Indaba X - Zambia - Speaker on the topic, The State of AI in Zambia.\n2021 DataCon Africa Speaker - The is a Pan-African yearly conference that invites data experts across Africa and the world over to tackle the burning topics in the data space. I was a speaker on the topic, “The Role of Data Culture When Integrating Tech and Business”."
  }
]