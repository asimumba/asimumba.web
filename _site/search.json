[
  {
    "objectID": "blog/pdf-data/index.html",
    "href": "blog/pdf-data/index.html",
    "title": "Breathing life back into PDF presented Data",
    "section": "",
    "text": "It is almost not surprising to find most of the summarised data is presented in the form of a report - whose format is mainly Portable Document Format (PDF). The challenge is when you would like to access that data in a dynamic format and form - where it can be analysed, reformatted and reshaped to your desire; a requirement which is hard, if not impossible to achieve with data presented in a PDF report. Trying to do so would be like wishing to extract water from a rock, which is an endeavour in futility.\nThe good news is, technology seem to run on a brain of its own. While one side of the technology spectrum impedes, another end liberate. One such solution to extracting the dead and static PDF presented data, is to turn to the powerful and versatile R package, tabulizer. The tabulizer package is an R wrapper for the powerful PDF extractor Java library Tabula. This package allows one to extract with ease, data presented in tables in a PDF document. For as long as the data is in a clean and uncluttered format. The extract_tables() function will try to guess the delimiters for the data and extract the data in the format which maintains close to the original data outline."
  },
  {
    "objectID": "blog/pdf-data/index.html#installation",
    "href": "blog/pdf-data/index.html#installation",
    "title": "Breathing life back into PDF presented Data",
    "section": "Installation",
    "text": "Installation\nFor the installation and usage, the package depends on Java. The appropriate Java Development Kit can be downloaded straight from the Oracle website here. Installation instructions are platform specific. Follow the instructions depending on your OS. I am on Windows, so I installed Java, running the jdk-8u144-windows-x64.exe executable file.\nInstalling tabulizer package, this can be installed from github. There is only the development version of the package, you will not find it on CRAN.\n\nif(!require(\"ghit\")){\n                       install.packages(\"ghit\")\n}\n# on 64-bit Windows\nghit::install_github(c(\"leeper/tabulizerjars\", \n                       \"leeper/tabulizer\"), \n                         INSTALL_opts = \"--no-multiarch\"\n                     )\n# elsewhere\nghit::install_github(c(\"leeper/tabulizerjars\", \n                       \"leeper/tabulizer\"))\n\nThis will download and install other Java related packages tabulizer depends on."
  },
  {
    "objectID": "blog/pdf-data/index.html#demo",
    "href": "blog/pdf-data/index.html#demo",
    "title": "Breathing life back into PDF presented Data",
    "section": "Demo",
    "text": "Demo\nFor demonstration purpose, I will use the report from the Central Statistics Office (CSO), Zambia, on Zambia Census Projection 2011-2035. Below is the outline of the sample data as presented in the PDF report.\n\n\n\nSample Data File - Source: CSO\n\n\nWe call the tabulizer package with the following command.\n\nlibrary(\"tabulizer\")\n\nThe main function is the extract_tables(). The first argument is the PDF file or report where the targeted table(s) is/are. The second argument is the pages, where you specify the page number the table of data is. There are other arguments such as area, which you can specify the targeted area(s) to extract. columns which matches with the number of pages to be extracted. This argument allows for each page extracted to be stored in its own separate column.The guess argument, which by default is =TRUE, allows for the function to guess the location of the table(s) on each page. For a list of all the arguments: run ?extract_tables in the R console. By default, the data is extracted as a list. Lists in R can be thought of as a vector containing other objects. We can zoom in on a particular object using the double square brackets,[[]]. For instance, the first object in the variable is indexed by the number 1, and the second object by 2, and so on. Since,only one table is being extracted, the variable below contain one column; extracted with this command,cso_table[[1]].\nThe default way, extract_table() extracts the data as a list of character matrices. This helps in cases where the data is irregular and cannot be properly coerced to a data frame (row by column format). To change this behaviour so that the extracted data is coerced to a data frame, we supply the method argument, and have data.frame as the value.\n\ncso <- (\"https://goo.gl/d2xMwS\")\n# This is the shortened version of the original URL.\n\ncso_table <- extract_tables(cso, pages = 24,\n                            method = \"data.frame\")\n# We are going to pass the cso variable to the extract_tables() function\ncso_column <- cso_table[[1]]\n\n# The table of interest is on page 24, the other arguments are left as defaults\n\nFrom the extracted results, it can be seen the output is not in a “tidy” format, to allow any meaning analyses to be done. The next step would be reshaping and reordering the extracted results into a neat data frame."
  },
  {
    "objectID": "blog/pdf-data/index.html#tidying-the-data",
    "href": "blog/pdf-data/index.html#tidying-the-data",
    "title": "Breathing life back into PDF presented Data",
    "section": "Tidying the data",
    "text": "Tidying the data\nTwo approaches can be implemented here: the easy way or the hard way.\n\nFirstly, the easy way. We can write the data to a CSV file and clean the data in Microsoft Excel. The solution is to use the write.csv() function. The first argument in the function is the data object. The file argument, you define the output file name together with the file extension - in our case it is a .CSV extension. The row.names specifies whether to include the default index R attaches to the data, which spans the length of your data.\n\n\n# I have passed a relative path where I want the CSV file to be stored\nwrite.csv(cso_column, file = \"cso_data.csv\",\n          row.names = FALSE) \n\nAfter cleaning the data in Excel, it can be re-imported to aid in analysis.\n\nSecond choice, the hard way. The tidyverse package has a suite of packages built specifically to handle such tasks. Thedplyr package, is one such package, which represents the grammar of data manipulation. Using well crafted verbs, one can transform, order, filter etc.. data with ease."
  },
  {
    "objectID": "blog/pdf-data/index.html#welcome-to-the-tidyverse",
    "href": "blog/pdf-data/index.html#welcome-to-the-tidyverse",
    "title": "Breathing life back into PDF presented Data",
    "section": "Welcome to the tidyverse",
    "text": "Welcome to the tidyverse\nFirst step is to clean the data, eliminating unwanted variables and title headers. That is in addition to transforming the data into a “tidy” format - A variable per column, observation per row, and a value per cell. The command below eliminates the first, second, and the last three row of the extracted data.\ntidyr package is used to gather the observations in the columns into rows and combine all the observations across 2 columns. The function gather() achieves this in the tidyr package.\nAfter gathering the data from the columns to rows, the second issue is to index the numbers by the corresponding provinces. This is achieved by replicating the provinces to span the length of the numbers. Combining the row names with their corresponding numbers completes our simple data extraction exercise.\n\ncso_data <- cso_data %>%\n  as_tibble()\n\ncso_provincial <- cso_data %>%\n  filter(sex == \"Total\") %>%\n  select(`2011`:`2035`) %>%\n  gather(key = \"year\", value = \"census_proj\")\n\nprovince <- rep(\n  c(\n    \"central\",\n    \"copperbelt\",\n    \"eastern\",\n    \"luapula\",\n    \"lusaka\",\n    \"muchinga\",\n    \"northern\",\n    \"north.western\",\n    \"southern\",\n    \"western\"\n  )\n  ,\n  6\n)\n\ncso_transformed <- cbind(cso_provincial, province) %>%\n  select(year, province, census_proj) %>%\n  as_tibble()\n\ncso_transformed\n\n# A tibble: 60 × 3\n   year  province      census_proj\n   <chr> <chr>         <chr>      \n 1 2011  central       1,355,775  \n 2 2011  copperbelt    2,143,413  \n 3 2011  eastern       1,628,880  \n 4 2011  luapula       1,015,629  \n 5 2011  lusaka        2,362,967  \n 6 2011  muchinga      749,449    \n 7 2011  northern      1,146,392  \n 8 2011  north.western 746,982    \n 9 2011  southern      1,642,757  \n10 2011  western       926,478    \n# … with 50 more rows\n\n\nFor the full data table view, see the table below.\n\nknitr::kable(cso_transformed, booktabs = TRUE,\n             caption = \"Census data per Province\")  \n\n\nCensus data per Province\n\n\nyear\nprovince\ncensus_proj\n\n\n\n\n2011\ncentral\n1,355,775\n\n\n2011\ncopperbelt\n2,143,413\n\n\n2011\neastern\n1,628,880\n\n\n2011\nluapula\n1,015,629\n\n\n2011\nlusaka\n2,362,967\n\n\n2011\nmuchinga\n749,449\n\n\n2011\nnorthern\n1,146,392\n\n\n2011\nnorth.western\n746,982\n\n\n2011\nsouthern\n1,642,757\n\n\n2011\nwestern\n926,478\n\n\n2015\ncentral\n1515086\n\n\n2015\ncopperbelt\n2362207\n\n\n2015\neastern\n1813445\n\n\n2015\nluapula\n1127453\n\n\n2015\nlusaka\n2777439\n\n\n2015\nmuchinga\n895058\n\n\n2015\nnorthern\n1304435\n\n\n2015\nnorth.western\n833818\n\n\n2015\nsouthern\n1853464\n\n\n2015\nwestern\n991500\n\n\n2020\ncentral\n1734601\n\n\n2020\ncopperbelt\n2669635\n\n\n2020\neastern\n2065590\n\n\n2020\nluapula\n1276608\n\n\n2020\nlusaka\n3360183\n\n\n2020\nmuchinga\n1095535\n\n\n2020\nnorthern\n1520004\n\n\n2020\nnorth.western\n950789\n\n\n2020\nsouthern\n2135794\n\n\n2020\nwestern\n1076683\n\n\n2025\ncentral\n1979202\n\n\n2025\ncopperbelt\n3016344\n\n\n2025\neastern\n2344980\n\n\n2025\nluapula\n1439877\n\n\n2025\nlusaka\n4004276\n\n\n2025\nmuchinga\n1326222\n\n\n2025\nnorthern\n1763638\n\n\n2025\nnorth.western\n1080072\n\n\n2025\nsouthern\n2445929\n\n\n2025\nwestern\n1173598\n\n\n2030\ncentral\n2254435\n\n\n2030\ncopperbelt\n3402007\n\n\n2030\neastern\n2655422\n\n\n2030\nluapula\n1623991\n\n\n2030\nlusaka\n4704135\n\n\n2030\nmuchinga\n1587414\n\n\n2030\nnorthern\n2040926\n\n\n2030\nnorth.western\n1227481\n\n\n2030\nsouthern\n2793523\n\n\n2030\nwestern\n1286880\n\n\n2035\ncentral\n2565450\n\n\n2035\ncopperbelt\n3823642\n\n\n2035\neastern\n3001152\n\n\n2035\nluapula\n1834667\n\n\n2035\nlusaka\n5465775\n\n\n2035\nmuchinga\n1879642\n\n\n2035\nnorthern\n2355007\n\n\n2035\nnorth.western\n1397137\n\n\n2035\nsouthern\n3184855\n\n\n2035\nwestern\n1416331\n\n\n\n\n\nWe can finally take a breather, and enjoy!\n\n\n\n\nvia GIPHY"
  },
  {
    "objectID": "blog/stock-returns/index.html",
    "href": "blog/stock-returns/index.html",
    "title": "Stocks Portfolio Analysis",
    "section": "",
    "text": "library(quantmod)\nlibrary(ggplot2) \nlibrary(xts)\nlibrary(highcharter)\nlibrary(PerformanceAnalytics)\nlibrary(dygraphs)\n\n\n\n\n\n## Dates have been loaded as a factor instead of as dates. \n#There is need to convert this variable to the date class and covert the rest of the variables to xts(Extensible Time Series)\n\ntemp <- xts(x =fundreturn[,-1], order.by = as.Date(fundreturn[,1])) \n# all the variables except data are coerced to xts using date as the index.\n# Overwriting the fundreturn object with the xts temp object\nfundreturn <- temp\nstr(fundreturn)\n\nAn 'xts' object on 2007-06-01/2016-11-01 containing:\n  Data: num [1:114, 1:18] 0.024 0.0057 0.0165 0.0126 -0.0301 0.0026 0.0045 0.0218 0.027 0.0315 ...\n - attr(*, \"dimnames\")=List of 2\n  ..$ : NULL\n  ..$ : chr [1:18] \"SPY\" \"IJS\" \"VTI\" \"IYR\" ...\n  Indexed by objects of class: [Date] TZ: UTC\n  xts Attributes:  \n NULL\n\nrm(temp) # getting rid of temp object\n## Next I, stack variables of the same unique class within the same column.\n#That is the returns of all the stocks in one column and the tickers in another column, indexed by the date variable.\n\n# make use of a temporary object.\ntemp <- data.frame(index(fundreturn), \n                   stack(as.data.frame(coredata(fundreturn))))\nfundreturn_final <- temp\n\n\n# Give the variables more descriptive names \n    names(fundreturn_final)[1] <-  \"Year\"\n    names(fundreturn_final)[2] <-  \"PercentageReturn\"\n    names(fundreturn_final)[3] <-  \"Stockticker\"\n    names(fundreturn_final)\n\n[1] \"Year\"             \"PercentageReturn\" \"Stockticker\"     \n\n  rm(temp) # removing the temp. object\n  # we coerce the data frame back to xts to be able to use quantmod and highcharter\n#fundreturn_final <- xts(x=fundreturn_final[-1], order.by = as.Date(fundreturn_final[,1]))\n\n\nggplot(data = fundreturn_final, aes(x=Year, \n                                    y=PercentageReturn, color=Stockticker)) +\n  geom_line()\n\n\n\n\n\n# Google stock Performance\n# GOOG\nGOOG <- subset(fundreturn_final, Stockticker==\"GOOG\")\n\ng <- ggplot(data = GOOG, aes(x=Year, y=PercentageReturn)) +\n  geom_line()\n# add features\ng <- g + ggtitle(\"Google Monthly Return\", \n                 subtitle = \"For the Period between June 2007 - Nov. 2016\") + \n  \n      theme(panel.background = element_rect(fill = \"white\",\n                                            colour = \"grey50\"),\n            axis.text = element_text(colour = \"blue\"),\n            \n            axis.title.y = element_text(size = rel(1.0), angle = 90),\n            axis.title.x = element_text(size = rel(1.0), angle = 360))\n\ng <- g + labs(x = \"Year\",\n        y =\"Return\") \n\ng + annotate(\"text\",x=as.Date(\"2009-09-01\"),\n             y=0.3245,label=\"HR\",\n             fontface=\"bold\",size=3, \n             colour = \"forestgreen\") +\nannotate(\"text\",x=as.Date(\"2010-04-01\"),\n         y=-0.1900,label=\"LR\",\n         fontface=\"bold\",size=3,\n         colour =\"red\") \n\n\n\n\n\n# Portfolio Performance Appraisal\n\n# Having the following portfolio\n\n# Google = GOOG, Amazon = AZMN,Apple = AAPL JP Morgans = JPM, Microsoft = MSFT, General Electric = GE, and Hewlett Packard = HPQ\n#, \"GE\", \"HPQ\"\n\np1 <- subset(fundreturn_final, Stockticker ==\"AMZN\")\np2 <- subset(fundreturn_final, Stockticker ==\"MSFT\")\np3 <- subset(fundreturn_final, Stockticker ==\"AAPL\")\np4 <- subset(fundreturn_final, Stockticker ==\"GOOG\")\n\nportfolio <- rbind(p1,p2,p3,p4) # binding the returns into one returns variable\nrm(\"p1\",\"p2\", \"p3\",\"p4\") # Removal of the temp. subsets \n \n# quick visual representation of the data\np <- ggplot(data = portfolio, aes(x = Year, y =PercentageReturn, colour = Stockticker))+geom_line()\np + labs(\n        x = \" Year\",\n        y = \"Return\",\n        colour = \"Stock ticker\") +\n    ggtitle(\" Apple, Amazon and Google Stock Returns\",subtitle =\" For the period June 2007 - Nov. 2016\")\n\n\n\n\n\n#Dygraphing\n\np1 <- subset(fundreturn_final, Stockticker ==\"AMZN\")\np2 <- subset(fundreturn_final, Stockticker ==\"MSFT\")\np3 <- subset(fundreturn_final, Stockticker ==\"AAPL\")\np4 <- subset(fundreturn_final, Stockticker ==\"GOOG\")\n\n\n# Converting to xts before graphing\nAMZN <- xts(x = p1[,c(-1,-3)], order.by = p1[,1])\nMSFT <- xts(x = p2[,c(-1,-3)], order.by = p2[,1])\nAAPL <- xts(x = p3[,c(-1,-3)], order.by = p3[,1])\nGOOG_ <- xts(x = p4[,c(-1,-3)], order.by = p4[,1])\n\nrm(\"p1\",\"p2\", \"p3\",\"p4\") # Removal of the temp. subsets \n\n\nmerged_returns <- merge.xts(AMZN,MSFT,AAPL,GOOG_) # merging the separate share returns into one xts object.\n\ndygraph(merged_returns, main = \"Amazon v Microsoft v Apple v Google\") %>% # Using pipes to connect the codes\n  dyAxis(\"y\", label =\"Return\") %>%\n  dyAxis(\"x\", label =\"Year\") %>%\n  dyOptions(colors = RColorBrewer::brewer.pal(4, \"Set2\")) \n\n\n\n\n\n\n# Let's now evaluate the portfolio\n\n## Part of the code is adapted from Jonathan Regenstein from RStudio\n\n# We assume an equally weighted portfolio. Allocating 25% to all the stocks in our portfolio\nw <- c(.25,.25,.25,.25)\n\n# We use the performanceAnalytics built infuction Return.porftolio to calculate portfolio monthly returns\n\nmonthly_P_return <-  Return.portfolio(R = merged_returns, weights = w)\n\n# Use dygraphs to chart the portfolio monthly returns.\ndygraph(monthly_P_return, main = \"Portfolio Monthly Return\") %>% \n  dyAxis(\"y\", label = \"Return\")\n\n\n\n\n\n\n# Add the wealth.index = TRUE argument and, instead of returning monthly returns,\n# the function will return the growth of $1 invested in the portfolio.\ndollar_growth <- Return.portfolio(merged_returns, weights = w, wealth.index = TRUE)\n\n# Use dygraphs to chart the growth of $1 in the portfolio.\ndygraph(dollar_growth, main = \"Growth of $1 Invested in Portfolio\") %>% \n  dyAxis(\"y\", label = \"$\")\n\n\n\n\n\n\n# Calculating the Sharpe Ratio\n# Taking the US 10 Year Treasury Rate of 2.40% as the risk free rate.\n# Making use of the built in SharpeRatio function in Performance Analytics package.\nprint(sharpe_ratio <- round(SharpeRatio(monthly_P_return, Rf = 0.024), 4))\n\n                                portfolio.returns\nStdDev Sharpe (Rf=2.4%, p=95%):           -0.0634\nVaR Sharpe (Rf=2.4%, p=95%):              -0.0424\nES Sharpe (Rf=2.4%, p=95%):               -0.0298"
  },
  {
    "objectID": "blog/tableau/index.html",
    "href": "blog/tableau/index.html",
    "title": "Demo: Embedding Tableau public Visualizations",
    "section": "",
    "text": "<iframe \nsrc=\"https://public.tableau.com/views/StockMarkets/LUSEKPI?:showVizHome=no&:embed=true\" width=\"652px\" height=\"756px\" style=\"border: 0px;\" scrolling=\"no\">\n</iframe>\nNOTE: You must strip off the extra text from the URL to the right side of the ? from the URL obtained from the Tableau public viz. There after add the colon followed by other arguments as indicated above.\nHere is the breakdown of each argument and value:\n\nsrc - This is the source URL from the tableau public visualisation dashboard.\nshowVizHome=no&:embed=true” - This allows the visualization to display on the external site its being embedded.\nwidth=“652px” height=“756px” - You can set the width to height ratio, expressed either as px or as %.\nstyle=“border: 0px;” - This eliminates the shadow border around the visualization.\nscrolling=“no” - setting this to no allows for the visualization to fully occupy the available space, and not have extended scrolling bars.\n\n\n\n\nThat is all. It is that simple to leverage Tableau public infrastructure for great visualizations which can be shared on private websites."
  },
  {
    "objectID": "blog/website/index.html",
    "href": "blog/website/index.html",
    "title": "Thoughts to Words - An Introduction to Aaron Simumba’s Website",
    "section": "",
    "text": "This post was originally submitted to the Rbind support website. ***"
  },
  {
    "objectID": "blog/website/index.html#introduction",
    "href": "blog/website/index.html#introduction",
    "title": "Thoughts to Words - An Introduction to Aaron Simumba’s Website",
    "section": "Introduction",
    "text": "Introduction\nHi, my name is Aaron Simumba, and you can find my website on the following URL: https://asimumba.rbind.io/. And on Github, plus you can follow me on Twitter. A brief background about myself: I often like to refer to myself as a “Lost Accountant” - because professionally I trained as an accountant, who lost his way to practise accounting and ended up into the data science space. I was excited to be in finance and accounting, I loved all the glory that came with working with financial data. But at heart, I loved computers and statistics. Statistics was probably one of my favourite courses at university. If there is one thing I got out of the accounting degree was that: “Debits should equal credits and vice versa”; and this has been my motivation to try to always find a reconciliation point for all I do."
  },
  {
    "objectID": "blog/website/index.html#blogging-journey",
    "href": "blog/website/index.html#blogging-journey",
    "title": "Thoughts to Words - An Introduction to Aaron Simumba’s Website",
    "section": "Blogging journey",
    "text": "Blogging journey\nI have always had a knack to express my thoughts. Previously I blogged with blogger and Wordpress. Mostly I blogged about random stuff that today I am ashamed to re-read. But that provided a very good foundation for harnessing my interest in blogging."
  },
  {
    "objectID": "blog/website/index.html#getting-lost-in-the-wilderness",
    "href": "blog/website/index.html#getting-lost-in-the-wilderness",
    "title": "Thoughts to Words - An Introduction to Aaron Simumba’s Website",
    "section": "Getting lost in the wilderness",
    "text": "Getting lost in the wilderness\nDuring my final undergraduate year, I so looked forward to finishing and venture into other interesting life persuasions. But that was one half of finishing all the university business - the other half involved writing my final year thesis. I was happy to finally get on with challenging my thought process. Unfortunately this enthusiasm was short lived. SPSS was in my way! I hated the work process of analysis in SPSS, and then copy the output to Microsoft word. Oh lord! cant there be something better? At the end of the day I went on with this process until the bitter end.\nPost university, I started looking for a better analysis environment, luckily I stumbled on R and all the tools in its ecosystem. And as you can guess I have not looked back. Been using R and its tools for now 1 year 7 months. If I’m to describe the experience, I would say, it’s been fun and challenging. With plenty of promises for the future."
  },
  {
    "objectID": "blog/website/index.html#website",
    "href": "blog/website/index.html#website",
    "title": "Thoughts to Words - An Introduction to Aaron Simumba’s Website",
    "section": "Website",
    "text": "Website\nIf there is one thing hard to be satisfied with is the visual aesthetics of the website. I first created my blogdown powered website in 2017 right around July. But between then and now, I have changed a minimum four website themes. This was mainly due the fact I had zero knowledge of HTML, let alone CSS. I took each theme as it came and simply replaced template filled sections with my own details. This left a hole which needed to be filled. I wanted to have control over even the most miniature website aspect. Fast forward, I crashed HTML and CSS courses to try and learn enough skills to be dangerous, of course in a minimal way. I want to thank Yihui Xie, from whom I have learnt a lot. His blog has great material to both his tools and his take on other open source software. Of course don’t forget to read the blogdown book.\nMy website is built on the hugo, github, netlify and blogdown platform. I am using the Beautiful Hugo theme. I like this theme for the following reasons: natively it comes with support for syntax highlighting, mathjax, a big image cover for the landing page and photoswipe, embedding a gallery of images is something I wanted.\n\n\n\nscreenshot\n\n\nThe theme has quite a clean design, which you can get started with a new website in minutes. Minimalistic design if you decide to strip out the images. Looking at the amount of work I did to achieve the visual effects as seen below. I must say I’m pleased and happy to call this a personal website!\n\n\n\nwebsite\n\n\nI broke and repositioned a lot of the native features to achieve the design as seen above. I let go of the avatar container on the navigation bar, re-aligned the navbar links to the right from their original left hand position. Added font awesome icons to some links.\n\n\n\nnavbar\n\n\nFor the landing page, I moved posts preview to the blog tab. And in addition, adjusted the big image of the landing page to cover the whole page. Resized the footer to accommodate the big front image.\nI love the post preview from the blog section. It gives a brief section to preview the content before delving fully into the post. From the original theme structure, this is presented on the front page if you use /content/post/ structure. To avoid this, I changed it to /content/blog/. All my posts reside in the /blog subdirectory.\n\n\n\nblog"
  },
  {
    "objectID": "blog/website/index.html#end-goal",
    "href": "blog/website/index.html#end-goal",
    "title": "Thoughts to Words - An Introduction to Aaron Simumba’s Website",
    "section": "End goal",
    "text": "End goal\nGoing forward, I will use this website to document my R learning path. And occasionally anything I find remotely interesting for the future me for reference purposes. More like turning my wild thoughts to written engravings. Happy blogging…\nI cannot document all the small but significant changes I made. The best option would be to fork the site and have a look at the changes made. For the source to my website, you can find it here\nEdit: This is a follow-up to my previous post"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDec 26, 2018\n\n\nAaron Simumba\n\n\n0 min\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\ndatahack4fi\n\n\ntwitter\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nNov 11, 2018\n\n\nAaron Simumba\n\n\n0 min\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nStock market\n\n\n\n\n\n\n\n\n\n\n\nNov 7, 2018\n\n\nAaron Simumba\n\n\n0 min\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nopen sourse\n\n\n\n\n\n\n\n\n\n\n\nFeb 24, 2018\n\n\nAaron Simumba\n\n\n1 min\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nblogdown\n\n\nR\n\n\nhugo\n\n\n\n\n\n\n\n\n\n\n\nFeb 19, 2018\n\n\nAaron Simumba\n\n\n4 min\n\n\n\n\n\n\n  \n\n\n\n\n\nThrough Tableau public integration\n\n\n\n\n\n\n\n\n\n\nNov 12, 2017\n\n\nAaron Simumba\n\n\n1 min\n\n\n\n\n\n\n  \n\n\n\n\n\nImplementation through Tabulizer\n\n\n\n\nrmarkdown\n\n\n\n\n\n\n\n\n\n\n\nOct 12, 2017\n\n\nAaron Simumba\n\n\n6 min\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOct 6, 2017\n\n\nAaron Simumba\n\n\n1 min\n\n\n\n\n\n\n  \n\n\n\n\n\nTwitter data mining\n\n\n\n\n\n\n\n\n\nOct 2, 2017\n\n\nAaron Simumba\n\n\n4 min\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSep 27, 2017\n\n\nAaron Simumba\n\n\n12 min\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSep 13, 2017\n\n\nAaron Simumba\n\n\n1 min\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nreflections\n\n\n\n\n\n\n\n\n\n\n\nAug 25, 2017\n\n\nAaron Simumba\n\n\n3 min\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nhugo\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nJul 3, 2017\n\n\nAaron Simumba\n\n\n1 min\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nApr 6, 2017\n\n\nAaron Simumba\n\n\n6 min\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\ngoogle\n\n\napple\n\n\nmsft\n\n\n\n\n\n\n\n\n\n\n\nMar 17, 2017\n\n\nAaron Simumba\n\n\n4 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "LICENSE.html",
    "href": "LICENSE.html",
    "title": "R . Data Science . Business Intelligence",
    "section": "",
    "text": "Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the “Software”), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\nThe above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\nTHE SOFTWARE IS PROVIDED “AS IS”, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE."
  },
  {
    "objectID": "portfolio.html",
    "href": "portfolio.html",
    "title": "Portfolio",
    "section": "",
    "text": "Sector Visualisation for BongoHive"
  },
  {
    "objectID": "talks.html",
    "href": "talks.html",
    "title": "Talks",
    "section": "",
    "text": "2017 Facebook Developer Circles - AI Masterclass speech 101 panelist under the topic: AI, is the Zambian Industry Ready for the next wave in tech, and what are the opportunities.\n2021 Deep Learning Indaba X - Zambia - Speaker on the topic, The State of AI in Zambia.\n2021 DataCon Africa Speaker - The is a Pan-African yearly conference that invites data experts across Africa and the world over to tackle the burning topics in the data space. I was a speaker on the topic, “The Role of Data Culture When Integrating Tech and Business”."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Get to know me",
    "section": "",
    "text": "I work in the financial services sector as a Business Intelligence & Analytics Lead. Previously, I worked for mPharma as a Data Management Associate.\nIn another life, I was an Accounting and Finance major. Graduated from the University of Namibia with a BA degree in Accounting(Honours). Equally tutored Business Administration students in the art of business and accounting at the University of Zambia.\nI am lost in the Data Science web… hoping to find my way out of the maze soon. I find data and the process of data analysis to glean insight from the raw data, overly exciting and fascinating alike.\nIn another world, I’m fascinated by stock markets and everything with a business and finance buzz.\nI can be found on Github poking around interesting projects… and occasionally I rant on twitter.\nFor this website\n\nThis will be a place where I share my daily musings in the world of R, analytics and mostly, everything remotely interesting. I have a knack for learning interesting data analytics technologies.\n\nLet's have fun! :smile:\nRecommended blogs\n\nYihui Xie\nDavid Robinson - Variance explained\nSimply Statistics\nR Bloggers\nRStudio blog\nRStudio community: Everything RStudio\nR Weekly"
  }
]