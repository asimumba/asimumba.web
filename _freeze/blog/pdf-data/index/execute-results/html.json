{
  "hash": "5dd001f5e8edc88bad7b07e5a7d2334d",
  "result": {
    "markdown": "---\ntitle: Breathing life back into PDF presented Data\nsubtitle: Implementation through Tabulizer\nauthor: Aaron Simumba\ndate: '2017-10-12'\ncategories: rmarkdown\ntags:\n  - pdf\n  - Java-jdk\n  - tabulizer\nslug: Breathing-life-back-into-PDF\nimage: \"pdf.jpg\"\n---\n\n::: {.cell}\n\n:::\n\n\nIt is almost not surprising to find most of the summarised data is presented in the form of a report - whose format is mainly Portable Document Format (PDF). The challenge is when you would like to access that data in a dynamic format and form - where it can be analysed, reformatted and reshaped to your desire; a requirement which is hard, if not impossible to achieve with data presented in a PDF report. Trying to do so would be like wishing to extract water from a rock, which is an endeavour in futility.\n\nThe good news is, technology seem to run on a brain of its own. While one side of the technology spectrum impedes, another end liberate. One such solution to extracting the dead and static PDF presented data, is to turn to the powerful and versatile R package, `tabulizer`. The [tabulizer](https://github.com/ropensci/tabulizer) package is an R wrapper for the powerful PDF extractor Java library  [Tabula](https://github.com/tabulapdf/tabula-java/). This package allows one to extract with ease, data presented in tables in a PDF document. For as long as the data is in a clean and uncluttered format. The `extract_tables()` function will try to guess the delimiters for the data and extract the data in the format which maintains close to the original data outline.\n\n## Installation\n\nFor the installation and usage, the package depends on Java.  The appropriate Java Development Kit can be downloaded straight from the Oracle website [here](http://www.oracle.com/technetwork/java/javase/downloads/index.html). Installation instructions are platform specific. Follow the instructions depending on your OS. I am on Windows, so I installed Java, running the `jdk-8u144-windows-x64.exe` executable file. \n\nInstalling tabulizer package, this can be installed from github. There is only the development version of the package, you will not find it on CRAN.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nif(!require(\"ghit\")){\n                       install.packages(\"ghit\")\n}\n# on 64-bit Windows\nghit::install_github(c(\"leeper/tabulizerjars\", \n                       \"leeper/tabulizer\"), \n                         INSTALL_opts = \"--no-multiarch\"\n                     )\n# elsewhere\nghit::install_github(c(\"leeper/tabulizerjars\", \n                       \"leeper/tabulizer\"))\n```\n:::\n\n\nThis will download and install other Java related packages tabulizer depends on.\n\n## Demo\n\nFor demonstration purpose, I will use the report from the Central Statistics Office (CSO), Zambia, on [Zambia Census Projection 2011-2035](https://www.zamstats.gov.zm/phocadownload/Zambia%20Census%20Projection%202011%20-%202035.pdf). Below is the outline of the sample data as presented in the PDF report. \n\n![Sample Data File - Source: CSO](https://user-images.githubusercontent.com/24398851/31639999-7d7cc290-b2e4-11e7-85d3-03cc648a73e6.png)\n\n\nWe call the tabulizer package with the following command.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(\"tabulizer\")\n```\n:::\n\n\nThe main function is the `extract_tables()`. The first argument is the PDF `file` or report where the targeted table(s) is/are. The second argument is the `pages`, where you specify the page number the table of data is. There are other arguments such as `area`, which you can specify the targeted area(s) to extract. `columns`   which matches with the number of pages to be extracted. This argument allows for each page extracted to be stored in its own separate column.The `guess` argument, which by default is `=TRUE`, allows for the function to guess the location of the table(s) on each page. For a list of all the arguments: run `?extract_tables` in the R console.\nBy default, the data is extracted as a list. Lists in R can be thought of as a vector containing other objects. We can zoom in on a particular object using the double square brackets,`[[]]`. For instance, the first object in the variable is indexed by the number 1, and the  second object by 2, and so on. Since,only one table is being extracted, the variable below contain one column; extracted with this command,`cso_table[[1]]`.\n\nThe default way, `extract_table()` extracts the data as a list of character matrices. This helps in cases where the data is irregular and cannot be properly coerced to a data frame (row by column format). To change this behaviour so that the extracted data is coerced to a data frame, we supply the `method` argument, and have `data.frame` as the value.   \n\n\n::: {.cell}\n\n```{.r .cell-code}\ncso <- (\"https://goo.gl/d2xMwS\")\n# This is the shortened version of the original URL.\n\ncso_table <- extract_tables(cso, pages = 24,\n                            method = \"data.frame\")\n# We are going to pass the cso variable to the extract_tables() function\ncso_column <- cso_table[[1]]\n\n# The table of interest is on page 24, the other arguments are left as defaults\n```\n:::\n\n\nFrom the extracted results, it can be seen the output is not in a \"tidy\" format, to allow any meaning analyses to be done. The next step would be reshaping and reordering the extracted results into a neat data frame. \n\n## Tidying the data\n\nTwo approaches can be implemented here: the easy way or the hard way.\n\n- Firstly, the easy way. We can write the data to a `CSV` file and clean the data in Microsoft Excel. The solution is to use the `write.csv()` function. The first argument in the function is the data object. The `file` argument, you define the output file name together with the file extension - in our case it is a `.CSV` extension. The `row.names` specifies whether to include the default index R attaches to the data, which spans the length of your data.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# I have passed a relative path where I want the CSV file to be stored\nwrite.csv(cso_column, file = \"cso_data.csv\",\n          row.names = FALSE) \n```\n:::\n\n\nAfter cleaning the data in Excel, it can be re-imported to aid in analysis.\n\n- Second choice, the hard way. The `tidyverse` package has a suite of packages built specifically to handle such tasks. The`dplyr` package, is one such package, which represents the grammar of data manipulation. Using well crafted verbs, one can transform, order, filter etc.. data with ease.\n\n## Welcome to the tidyverse\n\nFirst step is to clean the data, eliminating unwanted variables and title headers. That is in addition to transforming the data into a \"tidy\" format - A variable per column, observation per row, and a value per cell. The command below eliminates the first, second, and the last three row of the extracted data.\n\n\n\n\n\n`tidyr` package is used to gather the observations in the columns into rows and combine all the observations across 2 columns. The function `gather()` achieves this in the `tidyr` package.\n\nAfter gathering the data from the columns to rows, the second issue is to index the numbers by the corresponding provinces. This is achieved by replicating the provinces to span the length of the numbers. Combining the row names with their corresponding numbers completes our simple data extraction exercise.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncso_data <- cso_data %>%\n  as_tibble()\n\ncso_provincial <- cso_data %>%\n  filter(sex == \"Total\") %>%\n  select(`2011`:`2035`) %>%\n  gather(key = \"year\", value = \"census_proj\")\n\nprovince <- rep(\n  c(\n    \"central\",\n    \"copperbelt\",\n    \"eastern\",\n    \"luapula\",\n    \"lusaka\",\n    \"muchinga\",\n    \"northern\",\n    \"north.western\",\n    \"southern\",\n    \"western\"\n  )\n  ,\n  6\n)\n\ncso_transformed <- cbind(cso_provincial, province) %>%\n  select(year, province, census_proj) %>%\n  as_tibble()\n\ncso_transformed\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 60 × 3\n   year  province      census_proj\n   <chr> <chr>         <chr>      \n 1 2011  central       1,355,775  \n 2 2011  copperbelt    2,143,413  \n 3 2011  eastern       1,628,880  \n 4 2011  luapula       1,015,629  \n 5 2011  lusaka        2,362,967  \n 6 2011  muchinga      749,449    \n 7 2011  northern      1,146,392  \n 8 2011  north.western 746,982    \n 9 2011  southern      1,642,757  \n10 2011  western       926,478    \n# … with 50 more rows\n```\n:::\n:::\n\n\nFor the full data table view, see the table below.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nknitr::kable(cso_transformed, booktabs = TRUE,\n             caption = \"Census data per Province\")  \n```\n\n::: {.cell-output-display}\nTable: Census data per Province\n\n|year |province      |census_proj |\n|:----|:-------------|:-----------|\n|2011 |central       |1,355,775   |\n|2011 |copperbelt    |2,143,413   |\n|2011 |eastern       |1,628,880   |\n|2011 |luapula       |1,015,629   |\n|2011 |lusaka        |2,362,967   |\n|2011 |muchinga      |749,449     |\n|2011 |northern      |1,146,392   |\n|2011 |north.western |746,982     |\n|2011 |southern      |1,642,757   |\n|2011 |western       |926,478     |\n|2015 |central       |1515086     |\n|2015 |copperbelt    |2362207     |\n|2015 |eastern       |1813445     |\n|2015 |luapula       |1127453     |\n|2015 |lusaka        |2777439     |\n|2015 |muchinga      |895058      |\n|2015 |northern      |1304435     |\n|2015 |north.western |833818      |\n|2015 |southern      |1853464     |\n|2015 |western       |991500      |\n|2020 |central       |1734601     |\n|2020 |copperbelt    |2669635     |\n|2020 |eastern       |2065590     |\n|2020 |luapula       |1276608     |\n|2020 |lusaka        |3360183     |\n|2020 |muchinga      |1095535     |\n|2020 |northern      |1520004     |\n|2020 |north.western |950789      |\n|2020 |southern      |2135794     |\n|2020 |western       |1076683     |\n|2025 |central       |1979202     |\n|2025 |copperbelt    |3016344     |\n|2025 |eastern       |2344980     |\n|2025 |luapula       |1439877     |\n|2025 |lusaka        |4004276     |\n|2025 |muchinga      |1326222     |\n|2025 |northern      |1763638     |\n|2025 |north.western |1080072     |\n|2025 |southern      |2445929     |\n|2025 |western       |1173598     |\n|2030 |central       |2254435     |\n|2030 |copperbelt    |3402007     |\n|2030 |eastern       |2655422     |\n|2030 |luapula       |1623991     |\n|2030 |lusaka        |4704135     |\n|2030 |muchinga      |1587414     |\n|2030 |northern      |2040926     |\n|2030 |north.western |1227481     |\n|2030 |southern      |2793523     |\n|2030 |western       |1286880     |\n|2035 |central       |2565450     |\n|2035 |copperbelt    |3823642     |\n|2035 |eastern       |3001152     |\n|2035 |luapula       |1834667     |\n|2035 |lusaka        |5465775     |\n|2035 |muchinga      |1879642     |\n|2035 |northern      |2355007     |\n|2035 |north.western |1397137     |\n|2035 |southern      |3184855     |\n|2035 |western       |1416331     |\n:::\n:::\n\n\n\nWe can finally take a breather, and enjoy!\n\n\n<center>\n<iframe src=\"https://giphy.com/embed/13r9tgg7ZisiT6\" frameborder=\"0\"\n    width=\"300\" height=\"250\" allowtransparency class=\"giphy-embed\" allowFullScreen></iframe><p><a href=\"https://giphy.com/gifs/michelle-tanner-stickleyman-13r9tgg7ZisiT6\">via GIPHY</a></p>\n</center>\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}